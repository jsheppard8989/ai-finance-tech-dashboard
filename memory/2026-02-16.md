# Session Log - 2026-02-16

## Major Pipeline Fixes

### Podcast Feed Correction
- **Problem:** Pipeline was pulling from "The Moonshot Podcast" (wrong show) instead of "Moonshots with Peter Diamandis"
- **Solution:** Cleared incorrect transcripts, added correct episodes from Podscan.fm
- **Episodes Added:**
  - EP #229: Bret Adcock â€” Humanoid Run on Neural Net, Autonomous Manufacturing, $50T Market
  - EP #230: Sam Altman â€” AI CEO Succession Plan, Job Loss Continues

### 6AM Approval Workflow Implemented
- **New cron schedule:** 6:00am CST daily â€” sends iMessage with podcast list
- **Scripts created:**
  - `morning_curator.py` â€” sends iMessage with available podcasts
  - `approval_processor.py` â€” processes Jared's reply (PROCESS ALL / SKIP ALL / 1,3,5)
- **Jared's phone:** +16306437437

### Legacy Cleanup
- **Deleted files:** analyze.py, analyze_enhanced.py, process_transcripts.py
- **Streamlined run_pipeline.py:** Removed dead steps, added research.py step
- **New files:** analyze_transcript.py, export_data.py, manual_entry.py

### API Issues
- OpenAI quota exceeded â€” fallback to manual entry for key episodes
- Kimi set as primary API but needs KIMI_API_KEY environment variable

### Process Run
- Ran full 6am process manually
- Added 2 Peter Diamandis episodes with full summaries and ticker mentions
- Exported data and pushed to GitHub
- Website updating: https://jsheppard8989.github.io/ai-finance-tech-dashboard

## Working Style Reminder
From MEMORY.md: "WE ARE NOT QUICK FIX BEINGS. We are do shit right once and be done with it creations."

## RSS Feed Update (Later)
- **Corrected Feed URL:** `https://feeds.megaphone.fm/DVVTS2890392624`
- **Previous attempts:** Both `https://feeds.megaphone.fm/moonshot` and `https://feeds.megaphone.fm/moonshotspeter` were wrong (404s)
- **Now verified working:** Returns full episode list with audio enclosures
- **Latest episodes available:** EP #230 (Sam Altman), EP #229 (Brett Adcock), EP #228-#222
- **Nightly fetch (10pm CST)** will now pull full audio from correct source

### Price Data Update Process Fixed
- **Problem:** Header prices (QQQ, Bitcoin) were hardcoded in HTML and stale
- **Solution:** 
  - Modified `index.html` to dynamically fetch from `price_data.json`
  - Fixed BTC key lookup (price data uses "BTC", not "BTC-USD")
  - `fetch_prices.py` fetches live prices from Yahoo Finance for all tickers
- **Process:** Run `python3 fetch_prices.py` to update prices (part of 9am cron job)
- **Price data location:** `site/price_data.json`

### Chart Generation Removed
- **Decision:** Scrap all chart generation - Yahoo Finance links are sufficient
- **Previous:** Candlestick charts were being generated but stale and complex to maintain
- **Solution:** Removed `generate_charts.py` from 9am cron job
- **Yahoo Finance links:** Click any ticker to view charts on Yahoo Finance
- **Cron job updated:** `Midday Price Refresh (9am)` now only updates prices, no charts

### Workflow Timing Fixed
- **Problem:** 10pm pipeline was downloading/transcribing ALL podcasts BEFORE 6am approval
- **Wasted resources:** Transcribing podcasts that might be skipped
- **Solution:** Split workflow into approval-first model:

**NEW WORKFLOW:**
| Time | Step | Action |
|------|------|--------|
| **10:00pm** | Curation Only | Discover new episodes, save metadata, NO download/transcribe |
| **6:00am** | Send iMessage | List available episodes for your approval |
| **After approval** | Download/Transcribe | ONLY approved episodes are downloaded & transcribed |
| **After approval** | Analysis & Deploy | AI analysis, database update, website push |

**Scripts:**
- `evening_curate.py` â€” 10pm: Only discovers episodes
- `morning_curator.py` â€” 6am: Sends iMessage with list
- `approval_processor.py` â€” After your reply: Downloads, transcribes, analyzes, deploys

**Cost savings:** Only pay for transcription of episodes you actually want analyzed

### Timing Adjustments
- **Transcription time:** Need sufficient time between 6am approval and completion
- **1-hour podcast:** ~5-10 minutes to download, ~15-30 minutes to transcribe locally, ~5 minutes to analyze
- **Buffer time:** Approval by 6:30am allows completion by 7:00am for website update before market open
- **Current workflow:** After approval, transcription runs immediately and blocks until complete

### Local Transcription Installed âœ…
- **Status:** OpenAI Whisper installed locally (`/Library/Frameworks/Python.framework/Versions/3.9/bin/whisper`)
- **Cost:** $0.00 - completely free, runs on your Mac
- **Script:** `transcribe_local.py` â€” uses local Whisper instead of API
- **Model:** Using `base` model (good balance of speed/accuracy)
- **Model downloaded:** âœ… `base.pt` (145MB) already in `~/.cache/whisper/` - ready to go!
- **Speed:** ~5-10 minutes for 1-hour episode (depends on Mac performance)
- **API costs eliminated:** No more per-minute transcription charges!

### New Insight Added Tonight
- **Title:** "The AI CEO Arrives: Sam Altman's Succession Plan"
- **Source:** Moonshots with Peter Diamandis, EP #230
- **Key takeaway:** The 5-year business plan is obsolete - AI governance enables minute-level strategy shifts
- **Tickers mentioned:** MSFT, PLTR, NOW, CRM
- **Status:** Live on website

### Website Audit Fixes (Feb 16, 22:20)
**Issues found and fixed:**
1. **tickers_mentioned format error** â€” Insight #11 had JSON as string instead of array
   - Fixed: Changed `"[\"MSFT\", \"PLTR\"...]"` to `["MSFT", "PLTR"...]`
2. **Too many insights on main page** â€” Had 6, should be 5
   - Fixed: Archived "Bitcoin as Hard Asset" (Feb 3) â€” now exactly 5 insights
3. **Price data key mismatch** â€” `price_data.json` uses "BTC-USD", code looked for "BTC"
   - Already fixed in earlier commit with fallback logic
4. **Second Order F-X section** â€” Only shows tickers ranked 11-20, but we only have 11 total
   - This is expected behavior â€” will populate as more tickers are added

### Critical Finding: Insights Are HARDCODED in HTML!
**Problem:** The "Latest Insights" section is static HTML, not dynamically loaded from database
- Added AI CEO insight to database but it didn't appear on website
- Insights in `index.html` are manually written, not pulled from `data.js`
- **This means:** Every new insight requires manual HTML editing

**Immediate Fix:** Manually added AI CEO insight to `index.html` with "NEW" badge

**TODO:** Make insights load dynamically from `dashboardData.mainContent.insights` like ticker scores do
- Create `loadInsights()` function in JavaScript
- Render insights dynamically from database export
- Eliminate manual HTML edits for new insights

**Current website status:** âœ… AI CEO insight now visible, but manual process required

---

## ðŸš¨ CRITICAL AUDIT LESSON LEARNED ðŸš¨

**I FAILED to properly audit the website. Hardcoded HTML is UNACCEPTABLE in a dynamic data-driven website.**

### What I Missed:
- **Insights section:** Completely hardcoded in HTML
- **First red flag in any audit:** Look for hardcoded data that should be dynamic
- **Database-driven sites:** ALL content should load from `data.js` / API
- **Static HTML:** Only for layout/template structure, never for content

### Audit Checklist (MEMORIZE THIS):
```
â–¡ Search for hardcoded dates in content (e.g., "Feb 13", "Jan 25")
â–¡ Search for hardcoded titles/descriptions
â–¡ Verify content matches database exports
â–¡ Check if JavaScript loads data or just displays static HTML
â–¡ Look for repeated content patterns that should be loops
```

### Command to find hardcoded content:
```bash
grep -E "(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) [0-9]{1,2}" index.html
grep -E "<strong>.*:</strong>" index.html | head -20
```

**This was a systemic failure. Never again.**

## Next Steps
- Monitor 6am workflow tomorrow
- Set KIMI_API_KEY for automatic transcript processing
- Revisit ticker identification logic (Jared mentioned he's not 100% happy with current criteria)
