So we talked three years ago. I'm curious in your view. What has been the biggest update of the last three hours?
What has been the biggest difference between what I felt like last three years versus now?
Yeah, I would say actually the underlying technology like the exponential of the technology has has gone
Broadly speaking, I would say about about as I expected it to go. I mean, there's like plus or minus, you know, a couple.
There's plus or minus a year or two here. There's plus or minus a year or two there.
I don't know that I was predicted the specific direction of code.
But actually when I look at the exponential, it is roughly what I expected in terms of the March or the models from like, you know,
Smart High School student, a smart college student to like, you know, beginning to do PhD and professional stuff and in the case of code reaching beyond that.
So, you know, the frontier is a little bit uneven. It's roughly what I expected.
I will tell you though what the most surprising thing has been. The most surprising thing has been the lack of public recognition of how close we are to the end of the exponential.
To me, it is absolutely wild that, you know, you have people, you know, within the bubble and outside the bubble, you know,
But you have people talking about these these, you know, just the same tired old hot button political issues and like, you know,
Or around us, we're like, here's the end of the exponential. I want to understand what that exponential looks like right now because the first question I asked you when we recorded three years ago was, you know, what's up at scaling, how it might as a work.
And I have a similar question now, but I feel like it's a more complicated question because at least from the public's point of view, yes, three years ago there were these, you know,
Well, known public trends were across many of us in magnitude of compute, you can see how the loss improves. And now we have RL scaling and there's no publicly known scaling law for it.
It's not even clear what exactly the story is of is this supposed to be teaching the model skills, it's supposed to be teaching metal learning.
What is the scaling hypothesis at this point? Yeah, so, so I have actually the same hypothesis that I had, even all the way back in 2017.
So in 2017, I think I talked about it last time, but I wrote a doc called the big blob of compute hypothesis.
And you know, it wasn't about the scaling of language models in particular. When I wrote it, GPT1 had just come out, right?
So that was, you know, one among many things, right? There was back in those days, there was robotics, people tried to work on reasoning as a separate thing from language models.
There was scaling of the kind of RL that happened, you know, kind of happened in AlphaGo, and, you know, that that happened at Dodat, OpenAI, and, you know, people remember Starcraft at DeepMind, you know, the AlphaStar.
So it was written as a more general document. And the specific thing I said was the following that, and, you know, it's very, you know,
starting to put out the bit or less in a couple years later, but, you know, the hypothesis is basically the same.
So, so what it says is, all the cleverness, all the techniques, all the kind of, we need a new method to do something like that,
doesn't matter very much. There are only a few things that matter, and I think I listed seven of them.
One is like, how much raw compute you have. The other is the quantity of data that you have.
Then the third is kind of the quality and distribution of data, right?
It needs to be a broad, broad distribution of data. The fourth is, I think, how long you train for.
The fifth is, you need an objective function that can scale to the moon.
So the pre-training objective function is one such objective function, right?
Another objective function is, you know, the kind of RL objective function that says, like you have a goal,
you're going to go out and reach the goal. Within that, of course, there's objective rewards, like, you know, like you see in math and coding,
and there's more subjective rewards like you see in RL from human feedback or kind of higher order of versions of that.
And then the sixth and seventh were things around kind of like normalization or conditioning, like, you know,
just getting the numerical stability, so that kind of the big blob of compute flows in this laminar way,
instead of, instead of running into problems. So that was the hypothesis and it's hypothesis,
I still hold. I don't think I've seen very much that is not in line with that hypothesis.
And so the pre-trained scaling laws were one example of kind of what we see there.
And indeed, those have continued going. Like, you know, you know, I think now it's been,
it's been widely reported, like, you know, we feel good about pre-training.
Like, pre-training is continuing to give us gains. What has changed is that now we're also seeing the same thing for RL, right?
So we're seeing a pre-training phase and then we're seeing like an RL phase on top of that.
And with RL, it's actually just the same, like, you know, even even other companies have published,
like, you know, in some of their, in some of their releases have published things that say,
look, you know, we train the model on math contests, you know, AIME or the kind of other things.
And, you know, how well, how well the model does is log linear and how long we've trained it.
And we see that as well. And it's not just math contests. It's a wide variety of RL tasks.
And so we're seeing the same scaling in RL that we saw for pre-training.
You mentioned it's Richard Sutton and the bitter lesson. Yeah. I interviewed him last year.
And he is actually very non-LL un-pilled. And if I, I don't know if this is perspective, but one way to paraphrase this objection is something like, look,
something which possesses the true core of human learning would not require all these billions of dollars of data and compute
and this bespoke environments to learn how to use Excel or how to use PowerPoint and how to navigate a web browser.
And the fact that we have to build in these skills using these RL environments hints that we're actually lacking this core human learning algorithm.
And so we're scaling the wrong thing. And so yeah, that is a great question. Why are we doing all this RL scaling?
If we do think there's something that's going to be human-like and disability to learn on the fly.
Yeah. Yeah. So I think this kind of puts together several things that should be kind of thought of thought of differently.
Yeah. I think there is a genuine puzzle here, but it may not matter.
In fact, I would guess it probably, it probably doesn't matter. So let's take the RL out of it for a second.
Because I actually think RL and it's a red hearing to say that RL was any different from pre-training in this matter.
So if we look at pre-training scaling, it was very interesting back in 2017 when Alec Radford was doing GPT-1.
If you look at the models before GPT-1, they were trained on these datasets that didn't represent a wide distribution of text.
You had like, you know, these very standard, you know, kind of language modeling benchmarks.
And GPT-1 itself was trained on a bunch of, I think it was fanfiction actually.
But, you know, it was like literary text, which is a very small fraction of the text that you get.
And what we found with that, you know, and in those days it was like a billion words or something.
So small datasets and represented a pretty narrow distribution, right? Like a narrow distribution of kind of what you can see, what you can see in the world.
And it didn't generalize well. If you did better on, you know, the, you know, if I forgot what it was, some kind of fanfiction corpus, it wouldn't generalize that well to kind of the other tab.
You know, we had all these measures of like, you know, how well does the, how well does the model do it predicting all of these other kinds of texts.
You really didn't see the generalization.
It was only when you trained over all the tasks on the, you know, the internet.
When you, when you kind of did a general internet scrape, right, from something like, you know, common crawl or scraping links and read it, which is what we did for GPT 2.
It's only when you do that, that you kind of started to get generalization.
And I think we're seeing the same thing on RL that we're starting with first very simple RL tasks like training on math competitions.
Then we're kind of moving to, you know, kind of broader broader training that involves things like code as a task.
And now we're moving to do kind of many, many other tasks.
And then I think we're going to increasingly get generalization.
So that, that kind of takes out the RL versus the pre-training side of it.
But I think there is a puzzle here either way, which is that on pre-training.
When we train the model on pre-training, you know, we use like trillions of tokens, right?
And humans don't see trillions of words.
So there is an actual sample efficiency difference here.
There is actually something different that's happening here, which is that the model start from scratch.
And, you know, they have to get much more, much more training.
But we also see that once they're trained, if we give them a long context length,
they only think blocking a long context length is like inference.
But if we give them like a context length of a million, they're very good at learning the adapting within that context length.
And so I don't know the full answer to this.
But I think there's something going on that pre-training, it's not like the process of humans learning.
It's somewhere between the process of humans learning and the process of human evolution.
It's like it's somewhere between, like, we get many of our priors from evolution.
Our brain isn't just a blank slate, right?
The book's been written about.
I think the language models, they're much more blank slate.
They literally start as like random weights,
whereas the human brain starts with all these regions.
It's connected to all these inputs and outputs.
And so maybe we should think of pre-training and for that matter, RL as well,
as being something that exists in the middle space between human evolution and, you know, kind of human on the spot learning.
And as the in context learning that the models do,
as something between long-term human learning and short-term human learning.
So, you know, there's this hierarchy of, like, there's evolution, there's long-term learning,
there's short-term learning, and there's just human reaction.
And the LOM phases exist along this spectrum, but not necessarily exactly at the same points.
There's no analog to some of the human modes of learning that LOMs are kind of falling between the points.
Does that make sense?
Yes, although some things are still a bit confusing. For example, if the analogy is that this is like evolution,
so it's fine that it's not that sample efficient.
Then like, well, if we're going to get the kind of super sample efficient agent from in context learning,
why are we bothering to build in, you know, there's RL environment companies which are,
it seems like what they're doing is they're treating it, how to use this API, how to use Slack,
how to use whatever.
It's confusing to me why there's so much emphasis on that,
if the kind of agent that can just learn on the fly,
it's emerging where it's going to soon emerge or has already,
so I mean, I can't speak for the emphasis of anyone else.
I can only talk about how we think about it.
I think the way we think about it is,
the goal is not to teach the model every possible skill within RL,
just as we don't do that within pre-training, right?
Within pre-training, we're not trying to expose the model to, you know,
every possible, you know, way that words could be put together, right?
It's rather that the model trains on a lot of things,
and then it reaches generalization across pre-training, right?
That was the transition from GPT1 to GPT2 that I saw up close,
which is like, you know, the model reaches a point.
You know, I like had these moments where I was like,
oh yeah, you just give the model like,
you just give the model a list of numbers that's like, you know,
this is the cost of the house, this is the square feet of the house,
and the model completes the pattern and does linear regression, like,
not great, but it does it, but it's never seen that exact thing before.
And so, you know, to the extent that we are building these RL environments,
the goal is very similar to what was done five or 10 years ago with pre-training,
with we're trying to get a whole bunch of data,
not because we want to cover a specific document or a specific skill,
but because we want to generalize.
I mean, I think the framework you're laying down obviously makes sense,
like we're making progress for a CGI.
I think the crux is something like,
nobody at this point disagrees that we're going to achieve a CGI in the century.
And the crux is, you say we're hitting the end of the exponential,
and somebody else looks at this and says,
oh yeah, we're making progress, we've been making progress in 2012,
and then 2035 will have a human-like agent.
And so I want to understand what it is that you're seeing, which makes you think,
yeah, obviously we're seeing the kinds of things that evolution did,
or that human life time learning is like in these models.
And why think that it's one year away and not 10 years away?
Yeah, I actually think of it as like two cases to be made here,
or like two claims you could make, one of which is like,
stronger and the other of which is weaker.
So, I think starting with the weaker claim,
when I first saw the scaling back in like 2019,
I wasn't sure.
The whole this was kind of a 50-50 thing.
I thought I saw something that was,
and my claim was this is much more likely than anyone thinks it is.
Like this is wild, no one else would even consider this.
Maybe there's a 50% chance this happens.
On the basic hypothesis of, as you put it,
within 10 years will get to, you know,
what I call kind of country of geniuses at a data center.
I'm at like 90% on that.
And it's hard to go much higher than 90%
because the world is so unpredictable.
Maybe the irreducible uncertainty would be if we were at 95%,
where you get to things like, I don't know,
maybe multiple companies have, you know, kind of internal turmoil,
and nothing happens, and then Taiwan gets invaded
and all the fabs get blown up by missiles,
and you know, and that now you're going to have to stop.
You know, you know, you could construct a scenario
where there's like a 5% chance that it,
you know, you can construct a 5% world
where like things get delayed for 10 years.
That's maybe 5%.
There's another 5% which is that,
I'm very confident on tasks that can be verified.
So I think with coding,
I'm just except for that irreducible uncertainty,
there's just, I mean, I think we'll be there
on one or two years.
There's no way we will not be there in 10 years,
in terms of being able to do it end to end coding.
My one little bit, the one little bit of,
of fundamental uncertainty,
even on long time scales,
is this thing about tasks that aren't verifiable,
like planning a mission to Mars,
like, you know, doing some fundamental scientific discovery,
like CRISPR, like, you know,
writing a novel, hard to verify those tasks.
I am almost certain that we have a reliable path
to get there, but like,
if there was a little bit uncertainty, it's there.
So on the 10 years,
I'm like, you know,
90% which is about a certain as you can be.
Like I think it's, I think it's crazy
to say that this won't happen by, by 2035,
like in some sane world it would be outside the mainstream.
But the emphasis on verification
hints to me as a lack of,
a lack of belief that these models
would generalize.
If you think about humans,
you're better things that both,
which we get verifiable reward
and things which we don't.
You're like, you have to start.
We don't know.
This is why I'm almost sure.
We already see substantial generalization
from things that verify to things that don't,
we're already seen.
But it seems like you were emphasizing this
as a spectrum, which will split apart,
which doesn't mean to see more progress.
And I'm like,
but that doesn't seem like how it's getting.
But the world in which we don't make it,
or the world in which we don't get there,
is the world in which we do,
we do all the things that are verifiable
and then they like,
you know,
many of them generalize,
but what we kind of don't get fully there.
We don't,
we don't fully, you know,
we don't fully color in this side of the box.
It's, it's not a binary thing.
But it also seems to me,
even if even if it's in the world,
or generalization is weak,
when you only say to verify what domains,
it's not clear to me in such a world,
you could automate software engineering,
because software,
like in some sense,
you are,
quote unquote,
a software engineer.
Yeah.
But you part of being a software engineer for you,
involves writing these like,
long memos about your grand vision about different things.
And so I don't think that's part of the job of sweet.
That's part of the job of the company.
But I do think sweet involves like,
design documents and other things like that.
Which, by the way,
the models are not bad.
They're already pretty good at writing comments,
and so with,
again,
again, I'm making like much weaker claims here than I believe,
to like,
to kind of set up a,
you know,
to distinguish between two things.
Like, we're already almost there for software engineering.
And we are already almost there.
By what metric?
There's one metric,
which is like,
how many lines of code are written by AI?
And if you use,
if you consider other productivity improvements
in the course of the history of software engineering,
compilers write all the lines of software.
And, but we,
there's a difference between how many lines are written,
how many lines are written?
Yeah, yeah.
So I actually,
I actually agree with you on this.
So I've made this series of predictions on code
and software engineering.
And, and I think people have repeatedly kind of misunderstood them.
So let me, let me, let me, let me lay out the spectrum, right?
Like, I think it was like, you know, like,
you know,
eight or nine months ago or something,
I said, you know,
they,
I mean,
let me lay out the spectrum, right?
Like,
like,
like,
you know,
eight or nine months ago or something,
you know,
they, I model will be writing 90% of lines of code
in like,
you know,
three to six months,
which, which happened at least at some places, right?
Happened to happen to that
and Thropic happened with many people
downstream using our models.
But, but that's actually a very weak criterion, right?
People thought I was saying,
like,
we won't need 90% of the software engineers.
Those things are worlds apart, right?
Like,
I would put the spectrum as
90% of code is written by the model,
and that's a big difference in productivity.
90% of the end-to-end sweet tasks,
right?
Including things like compiling,
including things like setting up clusters,
and environments,
testing features,
writing memos,
90% of the sweet tasks are written by the models.
100% of today's sweet tasks are written by the models.
And even when when they have
doesn't mean software engineers are out of a job,
like,
there's like new higher level things they can do,
right?
They can manage.
And then there's a further down the spectrum,
like, you know,
there's 90% less demand for sweet,
which I think will happen,
but like,
this is a spectrum.
And, you know,
I wrote about it in the adolescents of technology,
where I went through this kind of spectrum with farming.
And so I actually totally agree with you on that.
It's just,
these are very different benchmarks from each other,
but we're proceeding through them super fast.
It seems like in part of your vision,
it's going from 90 to 100.
First, it's going to happen fast.
And two,
that somehow that leads to huge productivity improvements,
whereas when I notice,
even in greenfield products,
if people start with cloud code or something,
people report starting a lot of projects.
And I'm like,
do we see in the world out there
of Renaissance of software,
all these new features that wouldn't exist otherwise?
And at least so far,
it doesn't seem like we see that.
And so,
that doesn't make me wonder,
even if like,
I'd never had to intervene on cloud code.
There is this thing of like,
there's just,
the world is complicated,
jobs are complicated,
and closing the loop on self-contained systems,
whether it's just writing software or something.
How much broader gains,
we would see just from that.
And so,
maybe that makes us,
this should divert our estimation of the country of geniuses.
Well,
I actually,
I like simultaneously,
I simultaneously agree with you.
I agree that it's a reason why these things don't happen instantly,
but at the same time,
I think the effect is going to be very fast.
So, like,
I don't know,
you could have these two poles, right?
One is like,
you know,
AI is like,
you know,
it's not going to make progress,
it's slow,
like it's going to take,
you know,
kind of forever to diffuse within the economy,
right?
Economic diffusion has become one of these buzzwords,
that's like,
or a reason why we're not going to make AI progress,
or why AI progress doesn't matter.
And, you know,
the other axis is like,
we'll get recursive self-improvement,
you know,
the whole thing, you know,
can't you just draw an exponential line on the on the curve,
you know,
it's, you know,
we're going to have, you know,
Dyson spheres around the sun,
and like, you know,
you know,
so many nanoseconds after,
you know,
after after we get recursive,
I mean,
I'm completely caricaturing the view here,
but like,
you know,
there are these two extremes,
but what we've seen from,
from the beginning,
you know,
at least if you look within entropic,
this bizarre 10x per year,
growth and revenue that we've seen,
right? So, you know,
in 2023,
it was like 0 to 100 million.
2024,
it was 100 million to a billion.
2025,
it was a billion to like,
nine or 10 billion.
And then,
yes,
it's just about like a billion dollars
with your own products,
so you can just like,
have a clean 10,
and the first month of this year,
like that,
that exponential is,
you would think it would slow down,
but it would like,
you know,
we added another few billion
to like,
you know,
we added another few billion
to revenue in January.
And so,
you know,
obviously that curve can't go on forever,
right? You know,
the GDP is only so large.
I don't,
you know,
I would even guess that it bends,
that it bends somewhat this year,
but like,
that is like a fast curve, right?
That's like,
that's like a really fast curve,
and I would bet it stays pretty fast,
even as the scale goes to the entire economy.
So,
like,
I think we should be thinking about this middle world,
where things are like,
extremely fast,
but not instant,
where they take time,
because of economic diffusion,
because of the need to close the loop,
because,
you know,
it's like this fiddly,
oh man,
I have to do change management within my enterprise,
you know,
I have to like,
you know,
you know,
I like,
I set this up,
but you know,
I have to change the security permissions on this,
in order to make it actually work,
or you know,
I had this like old piece of software,
that, you know,
that like,
checks the model before it's compiled,
and like released,
and I have to rewrite it,
and yes,
the model can do that,
but I have to tell the model to do that,
it has to take time to do that.
And so,
I think everything we've seen so far
is compatible with the idea
that there's one fast exponential
that's the capability of the model,
and then there's another fast exponential
that's downstream of that,
the fusion of the model into the economy,
not instant,
not slow,
much faster than any previous technology,
but it has its limits.
And this is what we,
you know,
when I look inside andthropic,
when I look at our customers,
fast adoption,
but not infinitely fast.
Can I try a hot take on you?
Yeah,
I feel like diffusion is cope,
that people use to say when it's like,
if the model isn't able to do something,
they're like,
it's like a diffusion issue,
but then you should use the comparison to humans.
You would think that the inherent advantages
that AI's have would make diffusion
a much easier problem for new AI's getting onboarded,
than new humans getting onboarded,
so an AI can read your entire slack
and your drive in minutes.
They can share all the knowledge
that the other copies of the same instance have.
You don't have this adverse selection problem
when you're hiring AI's,
who's going to just hire copies of a VEDDI model.
Hiring a human is like so much more hassle,
and people hire humans all the time,
right?
We pay humans upwards of $50 trillion in wages
because they're useful,
even though it's like,
in principle it would be much easier
to integrate AI's into the economy
than it is to hire humans.
I think like the diffusion,
I feel like,
doesn't really explain.
I think diffusion is very real,
and doesn't have to,
you know,
doesn't exclusively have to do
with limitation limitations on the AI models.
Like again,
there are people who use diffusion
to, you know,
as kind of a buzzword to say this isn't a big deal.
I'm not talking about that.
I'm not talking about,
you know,
AI will diffuse at the speed that previous,
I think AI will diffuse much faster
than previous technologies have,
but not infinitely fast.
So I'll just give an example of this,
right?
Like there's like quad code.
Like quad code is extremely easy to set up.
You know,
if you're a developer,
you can kind of just start using quad code.
There is no reason why a developer
at a large enterprise
should not be adopting quad code
as quickly as, you know,
individual developer or developer
to start up.
And we do everything we can to promote it.
Right? We sell,
we sell quad code to enterprises
and big enterprises like,
you know, big financial companies,
big pharmaceutical companies,
all of them.
They're adopting quad code much faster
than enterprises typically adopt new technology.
Right?
But again, it takes time.
Like any given feature or any given product
like quad code or like co-work
will get adopted by the,
you know,
the individual developers who are on Twitter all the time
by the like series A startups
many months faster than,
you know,
then they will get adopted by like,
you know,
a large enterprise that does food sales.
There are a number of factors
like you have to go through legal.
You have to provision it for everyone.
It has to, you know,
like it has to pass security and compliance.
The leaders of the company who are further away
from the eye revolution,
you know,
are forward looking,
but they have to say,
oh,
it makes sense for us to spend 50 million.
This is what this quad code thing is.
This is why it helps our company.
This is why it makes us more productive.
And then they have to explain to the people two levels below
and they have to say,
okay,
we have 3,000 developers like,
how we're going to roll it out to our developers.
And we have conversations like this every day.
Like,
you know,
we are doing everything we can
to make anthropics revenue grow 20 or 30x a year instead of 10x a year.
You know,
and again,
you know,
many enterprises are just saying,
this is so productive.
Like,
you know,
we're going to take shortcuts
on our usual procurement process, right?
They're moving much faster than,
you know,
when we try to sell them just the ordinary API,
which many of them use,
the quad code is a more compelling product.
But it's not an infinitely compelling product.
And I don't think even AGI or Powerful AI
or country of geniuses in the data center
will be an infinitely compelling product.
It will be a compelling product enough
maybe to get 3 or 5 or 10x a year growth
even when you're in the hundreds of billions of dollars,
which is extremely hard to do.
And it has never been done in history before,
but not infinitely fast.
I buy that would be a slice though down.
And maybe this is not your claim.
But sometimes people talk about this like,
the abilities are there,
but because of diffusion,
otherwise we're basically at AGI
and then I don't believe we're basically at AGI.
I think if you had the country of geniuses in a data center,
if you were company to adopt the geniuses in a data center,
if you had the country of geniuses in a data center,
we would know it.
You would know it.
If you had the country of geniuses in a data center.
Like,
everyone in this room would know it.
Everyone in Washington would know it.
Like,
you know,
people in rural parts might not know it.
But like,
but we would know it.
We don't have that now.
But that's very clear.
As Daria was ending at,
the get generalization,
you need to train across a vibrating of realistic tasks
and environments.
For example,
with a sales agent,
the hardest part is in teaching it to mash buttons
in a specific database and sales force.
It's training the agent's judgment across ambiguous situations.
How do you sort through a database with thousands of leads
to figure out which ones are hot?
How do you actually reach out?
What do you do when you get ghosted?
When an AI lab wanted to train a sales agent,
Libobox brought in dozens of Fortune 500 sales people
to build a bunch of different oral environments.
They created thousands of scenarios
where the sales agent had to engage with the potential customer,
which was roleplayed by a second AI.
Libobox made sure that this customer AI had a few different personas.
Because when you cold call,
you have no idea who's going to be on the other end.
You need to be able to deal with a whole range of possibilities.
Libobox's sales experts monitor these conversations turned by turn,
tweaking the roleplaying agent to ensure that the kinds of things
and actual customer would do.
Libobox could iterate faster than anybody else in the industry.
This is super important because oral is an empirical science.
It's not a solve problem.
Libobox has a bunch of tools for monitoring agent performance in real time.
This lets their experts keep coming up with tasks
so that the model stays in the right distribution and difficulty
and gets the optimal rewards that don't during training.
Libobox can do this sort of thing in almost every domain.
They've got hedge fund managers,
radiologists, even airline pilots.
So whatever you're working on, Libobox can help.
Learn more at labelbox.com slash thorcash.
Coming back to concrete predictions because I think
because there's so many different things to disseminate.
It can be easy to talk past each other
when we're talking about capability.
So for example, when I interviewed three years ago,
I asked your prediction about what we should be expecting
three years from now.
I think you're right.
So when you said, we should expect systems
which if you talk them for the course of an hour,
it's hard to tell them apart from a generally well-educated human.
Yes.
And then you were right about that.
And I think spiritually I feel unsatisfied
because my internal expectation was
was that such a system could automate
large parts of white collar work.
And so it might be more productive to talk about
the actual and capabilities you want such a system.
So I will basically tell you what, you know,
where I think we are.
So let me ask you in a very specific question
so that we can figure out exactly what kinds of capabilities
we can do.
So maybe I'll ask about it in the context of a job I understand well,
not because it's the most relevant job
but just because I can evaluate the claims about it.
Take video editors, right?
I've video editors.
And part of their job involves learning
about our audience's preferences,
learning about my preferences and taste and the different
trade-offs we have.
And just over the course of many months building
up this understanding of context.
And so the skill and ability they have six months
a model that can pick up that skill on the job, on the fly.
When should we expect such an AI system?
Yeah.
So I guess what you're talking about is like,
you know, we're doing this interview for three hours
and then like, you know, someone's going to come in.
Someone's going to edit it.
They're going to be like, oh, you know,
you know, I don't know.
Dario like, you know, scratched his head and, you know,
we could edit that out and, you know,
don't know that.
There was this like long discussion
that like is less interesting to people.
And then, you know, then there's other thing that's like more
interesting to people.
So, you know, let's let's kind of make this this edit.
So, you know, I think the country geniuses in a data center
will be able to do that.
The way it will be able to do that is, you know,
it will have general control of a computer screen, right?
Like, you know, and you'll be able to feed this in
and it'll be able to also use the computer screen to like,
go on the web, look at all your previous,
look at all your previous interviews like,
look at what people are saying on Twitter
and response to your interviews like,
talk to you, ask you questions, talk to your staff,
look at the history of kind of edits and it's that you did
and from that like, do the job.
Yeah.
So, I think that's dependent on several things.
One that's dependent, and I think this is one of the things
that's actually blocking deployment,
getting to the point on computer use,
where the models are really masters
that using the computer, right?
And, you know, we've seen this climb in Benchmark's
and Benchmark's are always, you know,
imperfect measures, but like, you know,
less world is, you know, went from, you know, like 5%
of, you know, like, I think when we first released,
you know, computer use, like a year and a quarter ago
was like, maybe 15% or I don't remember exactly,
but we've climbed from that to like, 65% or 70%.
And, you know, there may be harder measures as well,
but I think computer use has to pass a point of reliability.
Okay, does that's a follow-up on that?
Yeah, before you move on to next point.
I often, for years, I've been trying to build different
internal LLM tools for myself,
and I often, I have these text-in text-out tasks,
which should be death center in the repertoire of these models.
And yet, I still hire humans to do them,
just because it's, if it's something like,
I didn't know if I were the best clips would be in this transcript,
and maybe they'll do like a 7 out of 10 job at them,
but there's not this ongoing way
I can engage with them to help them get better at the job,
the way I could with a human employee.
And so that missing ability,
you saw computer use with still block my ability to,
like, offload an actual job to them.
Again, there's, there's, this gets back to what,
to kind of what we were talking about before with learning on the job,
where it's, it's very interesting, you know,
I think, I think with the coding agents,
like, I don't think people would say that learning on the job
is what is, you know, preventing the coding agents from,
like, you know, doing everything end-hand.
Like, they keep, they keep getting better.
We have engineers and anthropic who, like, don't write any code.
And when I look at the productivity,
to your previous question, you know,
we have folks who say, this, this GPU kernel,
this chip, I used to write it myself,
I just have cloud do it.
And so there's, there's this enormous improvement in productivity.
And I don't know, like, when I see cloud code,
like familiarity with the code base or like,
you know, or, or a feeling that the model
hasn't worked at the company for a year,
that's not high up on the list of complaints I see.
And so I think what I'm saying is we're,
we're like, we're kind of taking a different path.
But no, don't you think with coding,
that's because there is an external scaffold of memory,
which exists in the code base,
which, I don't know how many other jobs have coding made
at fast progress precisely because it has its unique advantage
that other economic activity doesn't.
But, but when you say that,
what you're, what you're implying is that,
by reading the code base into the context,
I have everything that the human needed to learn on the job.
So that would be an example of whether it's written or not,
whether it's available or not,
a case where everything you needed to know,
you got from the context window, right?
And that, and that, what we think of as learning,
like, oh man, I started this job,
it's gonna take me six months to understand the code base,
the model just did it in the context.
I honestly don't know how to think about this,
because there are people who qualitatively report
where you're saying, there was a meter study,
I'm sure you saw last year.
Yes, where they had experienced developers
who tried to close the poor request
in repositories that they were familiar with.
And those developers reported an uplift.
They reported that they felt more productive
with the use of these models.
But in fact, if you look at their output
and how much was actually merged back in,
there's a 20% downlift.
And so I'm trying to square the qualitative feeling
that people feel with these models versus
one in a macro level where all the,
whereas it's like Renaissance of software.
And then two, when people do these independent evaluations,
why are we not seeing the,
yeah, so private event, if it's gonna be too expect.
Within anthropic, this is just really unambiguous, right?
We're under an incredible amount of commercial pressure
and making even harder for ourselves,
because we have all this safety stuff we do
that I think we do more than other companies.
So like the pressure to survive economically
while also keeping our values is just incredible.
We're trying to keep this 10x revenue curve going.
There's like there is zero time for bullshit.
There is zero time for feeling like we're productive
when we're not.
Like these tools make us a lot more productive.
Like why do you think we're concerned
about competitors using the tools?
Because we think we're ahead of the competitors.
And like we don't want to excel.
We wouldn't be going through all this trouble
if this was secretly reducing our productivity.
Like we see the end productivity every few months
in the form of model launches.
Like there's no kidding yourself about this.
Like the models make you more productive.
One, that people feeling like they're more productive
is qualitatively predicted by studies like this.
But too, if I just look at the end output,
obviously you guys are making fast progress.
But the fact, you know, the idea was supposed to be
with recursive improvement is that you make a better AI.
The AI helps people with the better next AI.
Yes, et cetera.
And when I see instead, if I look at the,
you open AI deep mind is that people are just
shifting around the podium every few months.
Maybe you think that stops because you've won or whatever.
But why are we not seeing the person with the best coding model
have this lasting advantage?
If, in fact, there are these enormous productivity gains
from the last coding model.
So no, no, no, I mean, I mean, I mean,
I think it's all like my model of a situation
is there's an advantage that's gradually growing.
Like I would say right now,
the coding models give maybe,
I don't know, a like 15, maybe 20% total factor speed up.
Like that's my view.
And six months ago, it was maybe 5%.
And so it didn't matter.
Like 5% doesn't register.
It's now just getting to the point where it's like
one of several factors that, that kind of matters.
And that's going to, that's going to keep speeding up.
And so I think six months ago, like, you know,
there were several, there were several companies that were
at roughly the same point because, you know,
this wasn't, this wasn't an notable factor,
but I think it started to speed up more and more.
You know, I would, I would also say,
there are multiple companies that, you know,
write models that are used for code.
And, you know, we're not perfectly good at,
you know, preventing some of these other companies
from, from using, from, from kind of using
our models internally.
So, you know, I think I think everything,
kind of, kind of everything we're seeing
is consistent with this kind of,
this kind of snowball model where, you know,
there's no hard, again, my, my, my, my,
my theme and all of this is like, all of this is soft,
take off like soft, smooth exponentials,
although the exponentials are relatively steep.
And so, and so we're seeing this snowball
gather momentum where it's like 10%, 20%, 25%,
you know, for, for 40%, and as you go,
yeah, and those all, you have to get all the,
like, things that are preventing you from,
from closing the loop out of the way,
but like, this is one of the biggest priorities
within Endropic.
Um, stepping back, I think,
before in this stack,
we were talking about, um,
well, when do we get this on the job learning?
And it seems like the coding,
the point you're making, the coding thing is,
we actually don't need on the job learning.
Uh, that you can have tremendous productivity improvements.
You can have potentially trillions of dollars
of revenue for eye companies,
without this basic human ability,
maybe that's not going to make sure to clarify.
Um, but without this basic human ability
to learn on the job,
but I just will go and like,
in, in most domains of economic activity,
people say,
I hired somebody,
they were in that useful for the first few months,
and then over time,
they built up the context,
understanding,
it's actually harder to find what we're talking about here.
But they got something.
And then now,
now they're,
they're powerhorse,
and they're so valuable to us.
And if AI doesn't develop
this ability to learn on the fly,
I'm not,
I'm a bit skeptical that we're going to see
huge changes to the world.
We've got that.
So I think, I think,
I think two things here,
right?
There's the state of the technology right now,
um,
which is again,
we have these two stages.
We have the pre-training and RL stage,
where you throw a bunch of data
and tasks into the models,
and then they generalize.
So it's like learning,
but it's like learning from more data
and, and not,
you know,
not learning over kind of one human or one model's
lifetime.
So again,
this is situated between evolution and,
and, and, and, and human learn.
But once you learn all those skills,
you have them.
And, and just like with pre-training,
just how the models know more,
you know,
if I look at a pre-trained model,
you know,
it knows more about the history of samurai
and Japan than I do.
It knows more about baseball than I do.
It knows,
you know,
it knows more about,
you know,
low-pass filters and electronics.
You know, all of these things,
it's knowledge is way broader than mine.
So I think,
I think even even just that,
you know,
you know,
kind of better at everything.
And then we also have again,
just with scaling the kind of existing setup,
we have the in-context learning,
which I would describe as kind of like human
on the job learning,
but like a little weaker and a little short term.
Like you look at in-context learning,
you give them a little bunch of examples.
It does get it.
There's real learning that happens in context.
And like a million tokens is a lot.
That's, you know,
that can be days of human learning, right?
You know,
if you think about the model,
you know,
kind of reading,
reading a million words,
you know,
it takes me how long would it take me to read a million.
I mean, you know,
like days or weeks at least.
So you have these two things,
and I think these two things within the existing paradigm,
may just be enough to get you the country's new assistant data center.
I don't know for sure,
but I think they're going to get you a large fraction of it.
There may be gaps,
but I certainly think,
just as things are,
this, I believe,
is enough to generate
trillions of dollars of revenue.
That's one.
That's all one.
Two is this idea of continual learning,
this idea of a single model
learning on the job.
I think we're working on that too.
And I think there's a good chance
that in the next year or two,
we also make,
we also solve that.
Again,
I,
you know,
I think you get most of the way there,
without it,
I think the trillions of dollars of,
you know,
the,
I think the trillions of dollars a year market,
maybe all of the national security implications
and the safety implications that I wrote about,
and that a lesson of technology can happen without it,
but I,
I also think we,
and I imagine others are working on it.
And I think there's a good chance
that,
that, you know,
that we get there within the next year or two.
There are a bunch of ideas.
I won't go into all of them in detail,
but, you know,
one is just,
make the context longer.
There's,
there's nothing preventing longer context from working.
You just have to train at longer context
and then learn to,
to serve them at inference.
And both of those are engineering problems
that we are working on,
and that I would assume others are working on as well.
Yeah.
So this context,
I think,
it seemed like there was a period from 2020 to 2023,
where from GBD-3 to GBD-4 to wherever,
there was an increase from,
like,
2000 context lying still,
128K.
If you're like,
for the next,
for the two-ish year since then,
in the sameish ballpark.
Yeah.
And when context lines get much longer than that,
people report qualitative degradation
in the ability of the model to consider that full context.
So I'm curious what you're,
internally saying,
that makes you think,
like,
oh, 10 million context,
to get human,
like, six months learning.
This isn't a research problem.
This is an engineering and inference problem,
right?
If you want to serve long context,
you have to like,
store your entire KV cache,
you know,
it's difficult to store all the memory in the GPUs,
to juggle the memory around.
I don't even know the detail,
you know,
at this point,
this is at a level of detail that,
that I'm no longer able to follow all the,
you know,
I knew it in the GPD-3 era of like,
you know,
these are the weights,
these are the activations you have to store.
But,
you know,
these days,
the whole thing is flip,
because we have MOE models,
and kind of all of that.
But,
and this degradation you're talking about,
like, again,
without getting too specific,
like a question I would ask is like,
there's two things.
There's the context length you train at,
and there's a context length that you serve at.
If you train at a small context length,
and then try to serve at a long context length,
like maybe you get these declarations.
It's better than nothing,
you might still offer it,
but you get these declarations,
and maybe it's harder to train at a long context length.
Yeah, so, you know,
there's a lot.
I want to,
at the same time,
ask about like,
maybe some rabbit holes of like,
well,
what would interest,
that if you had to train on longer context length,
that would mean that,
you're able to get sort of like less samples in,
for the same amount of compute,
but before,
maybe it's not worth diving deep on that.
I want to get an answer to the bigger picture question,
which is like,
okay,
so,
I don't feel a preference for a human editor,
that's been working for me for six months,
versus NEI,
that's been working with me for six months.
What year do you predict that that will be the case?
I,
I mean,
you know,
my guess for that is,
you know,
there's, there's a lot of problems that are basically like,
we can do this when we have the country of geniuses in the data center.
And so,
you know,
my, my, my picture for that is,
you know,
again,
if you,
if you,
if you, if you,
if you made me guess,
it's like one to two years,
maybe one to three years.
It's really hard to tell.
I have a,
I have a strong view,
99%,
95%,
yeah,
and then I have a hunch.
This is more like a 50,
50 thing that it's going to be more like one to two maybe more like one to three.
So one to three years,
contra Regina says just slightly less likely value,
will task a vetting video.
Oh,
it seems pretty,
economically valuable.
Let me tell you,
it's proportionally much,
it's just there are a lot of use cases like that.
Right, so,
you're predicting that within one to three years,
and in generally,
really 27, we will have a system that are quote, have the ability to navigate interfaces
available to humans doing digital work today, intellectual capabilities, mashing or exceeding
that of Nobel Prize winners, and the ability to interface with the physical world, and then
you give an interview two months ago with dealbook, where you're emphasizing your companies
more responsible compute scaling as compared to your competitors, and I'm trying to square
these two views where if you're really believed that we're going to have a country of
geniuses, you want as big a data center as you can get, there's no reason to slow down the
Tam of Nobel Prize winner that is actually, can do everything Nobel Prize winner can do is like
trillions of dollars, and so I'm trying to square this conservatism, which seems rational if you
have more moderate timelines with your stated views about AI progress. Yeah, so it actually
all fits together, and we go back to this fast, but not infinitely fast diffusion, so like
let's say that we're making progress at this rate, the technology is making progress this
fast. Again, I have very high conviction that it's going, we're going to get there within a
few years. I have a hunch that we're going to get there within a year or two, so a little uncertainty
on the technical side, but like pretty strong confidence that it won't be off by much.
What I'm less certain about is, again, the economic diffusion side, like I really do believe
that we could have models that are a country of geniuses, a country of geniuses in the data center
in one to two years. One question is, how many years after that do the trillions, do the trillions
and revenue start rolling in? I don't think it's guaranteed that it's going to be immediate.
I think it could be one year, it could be two years, I could even stretch it to five years,
although I'm skeptical of that, and so we have this uncertainty, which is even if the technology
goes as fast as I suspect that it will. We don't know exactly how fast it's going to drive
revenue. We know it's coming, but with the way you buy these data centers, if you're off by a couple
years, that can be ruinous. It is just like how I wrote, you know, in machines of loving grace,
I said, look, I think we might get this powerful AI of this country of genius in the data center,
that description you gave comes from the machines of loving grace. I said, we'll get that
2026, maybe 2027, again, that is my hunch wouldn't be surprised if I'm off by a year or two,
but that is my hunch. Let's say that happens. That's the starting gun.
How long does it take to cure all the disease? That's one of the ways that drives a huge
amount of economic value. You cure every disease. There's a question of how much of that goes to
the pharmaceutical company to the AI company, but there's an enormous consumer surplus because
everyone can get access for everyone, which I care about greatly. We cure all of these diseases.
How long does it take? You have to do the biological discovery. You have to, you have to, you have to,
you know, manufacture the new drug. You have to, you know, go through the regulatory
process. We saw this with like vaccines and COVID, right? Like, there's just this,
we got the vaccine out to everyone, but it took a year and a half. And so my question is,
how long does it take to get the cure for everything, which AI is the genius that can in theory
and that out to everyone? How long from when that AI first exists in the lab to when
diseases have actually been cured for everyone, right? You know, we've had a pollio vaccine for
50 years. We're still trying to eradicate it in the most remote corners of Africa. And, you know,
the Gates Foundation is trying as hard as they can. Others are trying as hard as they can,
but, you know, that's difficult. Again, I, you know, I don't expect most of the economic
diffusion to be as difficult as that, right? That's like the most difficult case. But,
but there's a, there's a real dilemma here. And where I've settled it on it is, it will be,
it will be faster than anything we've seen in the world, but it still has its limits.
And so then when we go to buying data centers, you know, you, again, again, the curve I'm looking at
is, okay, we, you know, we've had a 10x a year increase every year. So, beginning of this year,
we're looking at 10 billion in, in, in annual, in, you know, rate of annualized revenue with the
being of the year. We have to decide how much compute to buy. And, you know, it takes a year or two
to actually build out the data centers to reserve the data center. So, basically, I'm saying like in
2027, how much compute do I get? Well, I could assume that the revenue will continue growing 10x a year.
So, it will be one 100 billion at the end of 2026 and one trillion at the end of 2027. And so,
I could buy a trillion dollars. Actually, it would be like five trillion dollars of compute,
because it would be a trillion dollar a year for, for five years, right? I could buy a trillion
dollars of compute that starts at the end of 2027. And if my, if my revenue is not a trillion dollars,
if it's even 800 billion, there's no force on earth. There's, there's no hedge on earth that could
stop me from going bankrupt. If I, if I buy that much compute. And so, even though a part of my
brain wonders, if it's going to keep going 10x, I can't buy a trillion dollars a year of
compute in in in in in in in in in in in in in in in in 2027, if I'm just off by a year in that rate of growth,
or if the the growth rate is five x a year instead of 10x a year, then then, you know, the then you go
bankrupt. And and and and so you end up in a world where, you know, you're supporting hundreds of
billions, not trillions and you accepts you accept some risk that there's so much demand that
you can't support the revenue. And you accept still some risk that, you know, you got it wrong
and it's still so. And so when I talked about behaving responsibly, what I meant actually was not the
absolute amount. That that actually was not, you know, I think it is true we're spending somewhat
less than some of the other players. It's actually the other things like, have we been thoughtful about
it? Or are we yoloing and saying, oh, we're going to do $100 billion here, $100 billion is there.
I kind of get the impression that, you know, some of the other companies have not written down
the spreadsheet that they don't really understand the risks they're taking. They're just kind of doing
stuff because it sounds cool. And and we thought carefully about it, right? We're an enterprise business,
therefore, you know, we can rely more on revenue. It's less fickle than consumer. We have better
margins, which is the buffer between buying too much and buying too little. And so I think we bought
an amount that allows us to capture pretty strong upside worlds. It won't capture the full
10x a year. And things would have to go pretty badly for us to be for us to be in financial trouble.
So I think we thought carefully and we've made that balance. And that's what I mean when I say
that we're being responsible. Okay. So it seems like it's possible that we actually just have
different definitions of the country of a genius in a data center. Because when I think of like
actual human geniuses, an actual country of human geniuses in the data center, I'm like
I would happily buy five trillion dollars over the computer to run actual country of human
geniuses in the data center. So let's say JP Morgan or Moderna or whatever it doesn't want to use them.
Also, I've got a country of geniuses that they'll start their own company. And if like they can
start their own company and they're bottleneck by clinical trials, it is worth stating with clinical
trials like most clinical trials failed because the drug doesn't work. There's not efficacy, right?
And I make exactly that point in machines of love and grace. I say the clinical trials are going to
go much faster than we're used to. But not instantly, not infinitely fast. And then suppose it takes
a year to for the clinical trials to work out so that you're getting revenue from that and
you can make more drugs. Okay. Well, you've got a country of geniuses and you're an AI lab
and you have, you could use many more AI researchers. You also think there's these like
self reinforcing gains from, you know, smart people working on AI tech. So like, okay, you
can have that. That's right. But you can have the data center working on like AI progress.
Is there more gains from buying like substantially more gains from buying a trillion dollars a
year of compute versus 300 billion dollars a year of compute? If your competitor is buying
a trillion, yes, there is. Well, there's some gain, but then, but again, there's this chance that
they go bankrupt before, again, if you're off by only a year, you destroy yourselves. That's the
balance. We're buying a lot. We're buying a hell of a lot. Like we're buying an amount that's
comparable to that, that the biggest players in the game are buying. But if you're asking me,
why haven't we signed, you know, 10 trillion of compute starting in mid 2027? First of all,
can't be produced. There isn't that much in the world. But second, what is the country of geniuses
comes? But it comes in mid 2028 instead of mid 2027. You go bankrupt. So if your projection is
one to three years, it seems like you should have won 10 trillion dollars a compute by
2029, 2029, maybe 2028. It's like, I mean, you know, it seems like even in your the longest
version of the timelines you state, the compute you are ramping up to build doesn't seem, what makes
you think that? Well, as you said, you'll win the 10 trillion, like human wages, let's say are
on the order of 50 trillion a year. If you look at, so I won't, I won't talk about anthropic in
particular. But if you talk about the industry, like the amount of compute, the industry hit,
you know, the amount of compute the industry is building. This year is probably in the, you know,
I don't know, very low tens of, you know, call it 10 15 gigawatts. Next year, you know,
it goes up by roughly 3x a year. So like next year's third year, 40 gigawatts and 2028
might be a hundred, 2029 might be like 300 gigawatts and like each gigawatt costs like
maybe 10, I mean, I'm doing the math in my head, but each gigawatt costs maybe 10 billion dollars,
you know, or border 10 to 15 billion dollars a year. So, you know, you kind of, you know,
you put that all together and you're getting about about what you described, you're getting multiple
trillions a year by 2028 or 2029. So you're getting exactly that, you're getting exactly what
you predict. That's for the industry. That's for the industry. That's, that's for the industry.
That's what I suppose that anthropic compute keeps the reaxing a year and then by like 27, you have
or 27, 28, you have 10 gigawatts and like multiply that by, as you say, 10 billion. So then it's
like a hundred billion a year. But then you're saying the term by 2028. Again, I don't want to give
exact numbers for anthropic, but these numbers are too small. These numbers are too small.
Okay, interesting. I'm really proud that the puzzles I've worked on with Jane Street
have resulted in them hiring a bunch of people for my audience. Well, they're still hiring,
and they just send me another puzzle. For this one, they spent about 20,000 GPU hours training
backdoors into three different language models. Each one has a hidden prompt that elicits
completely different behavior. You just have to find the trigger. This is particularly cool,
because finding backdoors is actually an open question in front of your AI research.
And anthropic actually released a couple of papers about sleeper agents and they show that you can
build a simple classifier on the residual stream to detect when a backdoor is about to fire.
But they already knew what the triggers were because they built them. You don't. And it's
not feasible to check the activations for all possible trigger phrases. Unlike the other puzzles they
made for this podcast, Jane Street isn't even sure this one is solvable. But they've set aside
$50,000 for the best attempts and write-ups. The puzzles live at Jane Street dot com slash
to work as. And they're accepting submissions until April 1st. All right, back to Daria.
You've told investors that you plan to be profitable starting in 28. And this is the year
where we were potentially getting the country of geniuses at data center. And this is like
going to now unlock all this progress and medicine and health, et cetera, et cetera, and new
technologies. Wouldn't this be the exactly the time where you'd like want to reinvest in the business
and build bigger countries? I mean, profit of profitability is this kind of weird thing in this
field. I don't think, I don't think in this field, profitability is actually a measure of
kind of spending down versus investing in the business. Let's just take a model of this. I actually
think profitability happens when you underestimate the amount of demand you were going to get. And
loss happens when you overestimated the amount of demand you were going to get. Because you're buying
the data centers ahead of time. So think about it this way. Ideally, you would like, and again,
these are stylized facts. These numbers are not exact for them. I'm just trying to make a toy
model here. Let's say half of your compute is for training and half of your compute is for inference.
And the inference has some gross margin that's like more than 50%.
And so what that means is that if you were in steady state, you build a data center. If you knew
exactly exactly the demand you were getting, you would get a sort of amount of revenue. Say,
I don't know, let's say you pay $100 billion a year for compute. And on $50 billion dollars a year,
you support $150 billion on revenue and the other $50 billion are used for training.
So basically, you're profitable. You make 50 billion dollars a profit. Those are the economics
of the industry today. Or sorry, not today, but that's where we're projecting forward in a year or two.
The only thing that makes that not the case is if you get less demand than 50 billion,
then you have more than 50% of your data center for research and you're not profitable. So you
know, you train stronger models, but you're like not profitable. If you get more demand than you
thought, then your research gets squeezed, but you know, you're kind of able to support more
inference and you're more profitable. So it's maybe I'm not explaining it well, but the thing I'm
trying to say is you decide the amount of compute first. And then you have some target desire of
inference versus training, but that gets determined by demand. It doesn't get determined by
what I'm hearing is the reason you're predicting profit is that you are systematically
under investing in compute, right? Because if you actually, I'm saying, I'm saying it's hard to predict.
So these things about 20, 28 and what will happen, that's our attempt to do the best we can with investors.
All of this stuff is really uncertain because of the cone of uncertainty. Like, we could be profitable
in 2026 if the revenue grows fast enough. And then if we overestimate or underestimate the
next year, that could swing wildly. Like, what I'm trying to get is you have a modeling your head
of like the business invests, invests, invests, get scale, and kind of then becomes profitable.
There's a single point at which things turn around. I don't think the economics of this industry
work that way. I see. So if I'm understanding correctly, you're saying, because of the
discrepancy between the amount of compute, we should have gotten and the amount of compute we got,
we were like sort of forced to make profit. But that doesn't mean we're going to continue making
profit. We're going to like reinvest the money because, well, now he has made so much progress and we
want the bigger country of geniuses. And so then back into revenue is high, but losses are also high.
If we, if we predict, if every year, we predict exactly what the demand is going to be,
we'll be profitable every year. Because, because spending spending 50% of your compute on 50% of
your compute on research, roughly, plus a gross margin that's higher than 50% and correct demand
prediction leads to profit. That's the profitable business model that I think is kind of like
there, but like obscured by these like building ahead in prediction errors. I guess you're treating
the 50% as a, as a sort of like, you know, just like a given constant. Whereas you, in fact,
if you, if the I progress is fast and you can increase the progress by scaling up more, you just have
more than the 50% and not make progress. Here's what I'll say. You might want to scale up it more.
You might want to scale it up more, but, but you know, remember the log returns to scale, right?
If, if 70% would get you a, a very little bit of a smaller model through a factor of 1.4x,
right? Like that extra $20 billion is, is, is, is, is, you know, that each each dollar there
is worth much less to you because it because the log linear set up. And so you might find that
it's better to invest that, that, that, that, that, it's better to invest that $20 billion in, you know,
in, in serving inference or in hiring engineers who are who are kind of better, who are
kind of better, who are kind of better or what they're doing. So the, the re-nice said 50%.
That's not, that's not exactly our target. It's not exactly going to be 50%. It'll probably
vary very, very over time. What, what I'm saying is the, the, the, the, the, the, the like log linear return,
what it leads to is, you spend of order 1 fraction of the business, right? Like not,
5% not 95% and then it, then it, then it, that, you know, then, then that you get diminishing returns
because of the, because of the log linear set up, like convincing Dario, but like believing AI
progressers, I've met, like, okay, you, you don't invest in research because it has diminishing
returns, but you invest in the other things you mentioned. Again, again, we're talking about diminishing
returns after you're spending 50 billion a year, right? Like, this is a point I'm sure you'd
make, but like diminishing returns on a genius is it could be quite high. And more generally,
like, what is profit in the market economy, profit is basically saying the other companies in
the market can, like, do more, but more things with this money that I can put aside and drop,
I'm just trying to, like, because I, you know, I don't want to give information about
anthropic is why I'm giving these stylized numbers, but like, let's just derive the equilibrium
of the industry, right? I think the, so, so, so, so why doesn't everyone spend 100% of their,
you know, 100% of their compute on training and not serve any customers, right? It's because
if they didn't get any revenue, they couldn't raise money, they couldn't do compute deals,
they couldn't buy more compute the next year. So, there's going to be an equilibrium where every
every company spends less than 100% on, on, on, on, on training and certainly less than 100%
on inference, it should be clear why you don't just serve the current models and, you know,
and, and, and, and, and, and never train another model because then, you don't have any demand
because you'll, because you'll fall behind. So, there's some equilibrium. It's, it's not going
to be 10%, it's not going to be 90%, let's just say as a stylized fact, it's 50%. That's what I'm
getting at. And, and, and, and, and I think we're going to be in a position where that equilibrium
of how much you spend on training is less than the gross margins that, that, that, that, that,
that you're able to get on compute. And so, the, the, the, the underlying economics are profitable.
The problem is you have this, this hellish demand prediction problem when you're, when you're buying
the next year of compute and you might guess under and be very profitable, but have no compute
for research or you might guess over and, you know, you're, you're, you're, you, you are not profitable
and you have all the compute, it could compute for research and work. Does, does, does, does that make sense?
Does it just does dynamic model of industry? Yeah. And maybe, maybe stepping back, I'm like,
I, I'm not saying, I, I think the country of geniuses is going to come in two years and therefore,
you should buy this compute. To me, what you're saying, the end conclusion you're arriving at makes a lot
of sense, but that's because it's like, oh, it seems like country geniuses is hard and there's a long
way to go. And so the stepping back, the thing I'm trying to get at is more like,
it seems like your world is compatible with somebody who says, we're like 10 years away from
a world in which like we're generating trillion dollars. That's just, that's just not my view.
Yeah. That is, that is, that is not my view. Like, I, so, so all like, I'll like make another
prediction. It is hard for me to see that there won't be trillions of dollars in revenue before 2030.
Like, I can, I can construct a plausible world. It takes maybe three years, so that, you know,
that would be the end of what I think it's plausible. Like in 2028, we get the real country of geniuses
in the data center. You know, the revenue has been, you know, the revenue has been going into the
maybe as it is in the low hundreds of billions by by by by by 2028. And, and then the country of
geniuses accelerates it to trillions, you know, and, and basically, we're basically on the slow
end of diffusion. It takes two years to get to the trillions. That, that would, that, that, that would
be the world where it takes until, that would be the world where it takes until 2030. I, I suspect,
even composing the technical exponential and diffusion exponential, we'll get there before 2030.
So, you let out a model where anthropic makes profit because it seems like fundamentally
we're in a compute constrained world and so it's like eventually we keep growing compute.
No, I think I think the way the profit comes is again, and, and, you know, let's just
abstract the whole industry here. Like we have a, you know, let's just imagine we're, we're in
like an economics textbook. We have a small number of firms. Each can invest a limited amount
in, you know, or, or like each can invest some fraction in R&D. They have some marginal cost to
serve. The margins on that, the profit margin, the gross profit margins on that marginal cost
are like very high because because because because inference is efficient, there's some competition,
but the models are also differentiated. There's some, there's some, you know, companies will compete
to push their research budgets up, but like because there's a small number of players, you know,
we have the, what is it called, and I can decarno equilibrium. I think is what the, what the
small number of firm equilibrium is. The point is it doesn't equilibrate to perfect competition
with with with with with with with with with with zero margins. If there's like three firms,
if there's three firms in the economy, all are kind of independently behaving, behaving rationally,
it doesn't equilibrate to zero. Help me understand that because right now we do have three
leading firms and they're not making profit. And so what, yeah, what is changing?
Yeah, so the, again, the gross margins right now are very positive. What's happening is a combination
of two things. One is we're still in the exponential scale up phase of compute. Yeah. So what
basically what that means is we're training like a model gets trained. Yeah. It costs, you know,
let's say a model got trained that costs a billion dollars last year. And then this year,
it produced $4 billion of revenue and cost $1 billion to, to, to, to, to inference from.
So, you know, again, I'm using stylized number here, but you know, the 75% you know, gross gross
margins and, you know, this, this 25% tax. So that model as a whole makes $2 billion. But at the
same time, we're spending $10 billion to train the next model because there's an exponential scale up.
And so the company loses money. Each model makes money, but the company loses money. The equilibrium
I'm talking about is an equilibrium where we have the country of geniuses. We have the country of geniuses
in the data center, but that model training scale up has a quilibrated more. Maybe it's still,
it's still going up. We're still trying to predict the demand, but it's more, it's more leveled out.
I'll give you a few things there. So let's start with the current world. In the current world,
you're right that, as you said before, if you treat each individual model as a company, it's
profitable. Of course, a big part of the production function of being a frontier lab is training
the next model, right? So if you didn't do that, then you'd make profit for two months.
That's right. And you wouldn't have margins because you wouldn't have the best model. And then so
yeah, you can make profits too much in terms of that. At some point, that reaches the biggest scale
that it can reach. And then in equilibrium, we have algorithmic improvements, but we're spending
roughly the same amount to train the next model as, as we, as we spend to train the current model.
So this equilibrium relies, I mean at some point, it's at some point you run out of money in the
economy. The fixed length of labor follows, the economy's going to grow, right? That's one of your
predictions. Well, we're going to have this, but this is another example of the theme I was talking
about, which is that the economy will grow much faster with AI, then I think it ever has before,
but it's not like right now the computer's growing three X a year. I don't believe the economy
is going to grow 300% a year. Like I said, this in machines of loving grace, like I think
we may get 10 or 20% per year growth in the economy, but we're not going to get 300% growth in
the economy. So I think I think in the end, you know, if compute becomes the majority of what the
economy produces, it's going to be kept by that. So let's, okay, now let's assume a model where
compute statescapped. Yeah, the world where frontier labs are making money is one where they continue
to make fast progress. Because fundamentally, your margin is limited by how good the alternative is.
And so you are able to make money because you have a frontier model, if you did not have a frontier
model, you wouldn't be making money. And so this model requires, they're never to be a steady state.
Like forever and ever, you actually making more algorithmic progress. I don't think that's true.
I mean, I feel like we're like, we're taught, we're, you know, we're, you know, we're,
I feel like this is an economic stuff. Like, you know, this is like an economic,
you know, that's how we call it. We never stop talking about economics. We never stop talking
about economics. So no, but, but there, there are, there are worlds in which, um, you know,
there, so I, I don't think this field's going to be, I don't think this field's going to be
a monopoly. Oh, my lawyers never want me to say it word phenomenally. But I don't think this
field's going to be a monopoly, but you do get, you get industries in which there are small
number of players, not one, but a small number of players. And or narrowly, like the way you
get monopolies like Facebook or, or meta, I always call it Facebook, but um, uh, is, is these kind
of net, is these kind of, these kind of network effects. The way you get industries in which there are
small number of players are very high-class of entry, right? Um, so, you know, a cloud is like this.
I think cloud is a good example of this. You have three, maybe four players within cloud. I think
I think that's the same gray eye. Three, maybe four. Um, and the reason is that it's, it's so expensive.
It requires so much expertise and so much capital to like run a cloud company, right? And so you
have to put up all this capital and then in addition to putting up all this capital, you have to
get all of this other stuff that like, you know, requires a lot of skill to, you know, to make it
happen. And so it's like, if you go to someone and you're like, I want to disrupt this industry,
here's $100 billion, or you're like, okay, I'm putting $100 billion and also betting that you
can do all of these other things that these people have been doing. One of you creates a profit in the
industry. Yeah. And, and then, and then the effect of your entry is, this is the profit margins.
Go down. So, you know, we have equilibrium like this all the time in the economy where we have a few,
we have a few players, profits are not astronomical, margins are not astronomical, but they're, they're
not zero, right? And, and, you know, I think, I think that's what we see on cloud, cloud is very
undifferentiated. Models are more differentiated than cloud, right? Like, everyone knows,
cloud is, cloud is good at different things than GPT is good at is then, then Gemini is good at.
And it's not just cloud's good at coding, GPT is good at, you know, math and reasoning, you know,
it's more subtle than that. Like, models are good at different types of coding. Models have different
styles. Like, I think, I think these things are actually, you know, quite different from each other.
And so, I would expect more differentiation than you see in, in cloud. Now, there, there actually
is a counter, there, there is one counter argument. And that counter argument is that if all of that,
the process of producing models becomes, if AI models can do that themselves, then that could
spread throughout the economy. But that is not an argument for commoditizing AI models in general,
that's kind of an argument for commoditizing the whole economy at once. I don't know what,
what quite happens in that world, where basically anyone can do anything, anyone can build anything,
and there's like no mode around anything at all. I mean, I don't know, maybe we want that world.
Like, like, maybe that's the, maybe that's the end state here. Like, maybe, maybe, you know,
when maybe when when when kind of AI models can do, you know, when when when when when when
AI models can do everything, if we've solved all the safety and security problems like, you know,
that's one of the one of the mechanisms for, you know, you know, just just kind of the economy
flattened itself again. But, but that's kind of like post like far post countries, this isn't a
data center. Maybe a finer way to put that potential point is one, it seems like AI research
is especially loaded on raw intellectual power, which will be especially a button in the
world with the CGI. And too, if you just look at the world today, there's very few technologies
that seem to be diffusing as fast as as AI algorithmic progress. And so the does hint that this
industry sort of structurally diffusive. So I think coding is going fast, but I think AI research
is a super set of coding and their aspects of it that are not going fast. But I do think again,
once we get coding, once we get AI models going fast, then, you know, AI, you know, that will speed
up the ability of AI models to kind of do everything else. So I think while coding is going fast now,
I think once the AI models are building the next AI models and building everything else,
the whole economy will kind of go at the same pace. I am, I am worried geographically though.
I'm a little worried that like just proximity to AI, having heard about AI that that may be
one differentiator. And so when I said the like, you know, 10 or 20 percent growth rate,
a worry I have is that the growth rate could be like 50 percent in Silicon Valley and, you know,
parts of the world that are kind of socially connected to Silicon Valley. And, you know,
not that much faster than its current pace elsewhere. And I think that'd be a pretty messed up world.
So one of the things I think about a lot is how to prevent that. Yeah. Do you think that once we
have the Center of Geniuses of the Data Center that robotics is sort of quickly solved after
words because it seems like a big problem with the robotics is that a human can learn how to
teleoperate current hardware, but current AI models can't at least not in a way that's super productive.
And so if we have this ability to learn like a human, should it solve robotics immediately?
Yeah. I don't think it's dependent on learning like a human. It could happen in different ways.
Again, we could have trained the model on many different video games, which are like robotic
controls or many different simulated robotics environments or just, you know, train them to control
computer screens and they learn to generalize. So it will happen, it's not necessarily dependent
on human-like learning. Human-like learning is one way. It could happen if the model is like,
oh, I pick up a robot. I don't know how to use it. I learn that could happen because we discovered
a discovering continual learning. That could also happen because we train the model on a bunch of
environments and then generalize or it could happen because the model learns that in the context
it doesn't actually matter which way. If we go back to the discussion we had like an hour ago,
that type of thing can happen in that type of thing can happen in several different ways.
But I do think when for whatever reason the models have those skills, then robotics will be
revolutionized, both the design of robots because the models will be much better than humans at that.
And also the ability to kind of control robots. So we'll get better at the physical building,
the physical hardware building, the physical robots, and we'll also get better at controlling it.
Now does that mean the robotics industry will? So we generating trillions of dollars
of revenue. My answer there is yes, but there will be the same extremely fast but not infinitely
fast diffusion. So will robotics be revolutionized? Yeah, maybe tack on another year or two.
That's the way I think about these things. There's a general skepticism about
extremely fast progress. Here's maybe, which is like it sounds like you are going to solve
continue learning what to learn other within the matter of years. But just as people weren't
talking about continue learning a couple of years ago and then we realized, oh why are these models
as useful as they could be right now, even though they are clearly passing the touring test and
are experts in somebody who do it for domains, maybe it's this thing. And then we solve this thing
and we realize actually there's another thing that human intelligence can do and that's a basis
of human labor that these models can't do. And then why not think there will be more things like this?
So I think that like, we've found the pieces of human intelligence. Well, to be clear, I mean,
I think continue learning as I said before might not be a barrier at all. Like, like,
you know, I think we may be just get there by pre-training generalization and our
generalization. Like, I think there might not be, there basically might not be such a thing at all.
In fact, I would point to the history in ML of people coming up with things that are barriers that
end up kind of dissolving within the big blob of compute, right, that people talked about, you know,
you know, how do you have, you know, how do you how do your models keep track of nouns and
verbs and, you know, how do they, you know, they can understand cement syntactically,
but they can't understand semantically. You know, it's only statistical correlations. You can
understand a paragraph. You can understand a word. There's reasoning. You can't do reasoning,
but then suddenly it turns out you can do code and math very well at all. So I think there's,
there's actually a stronger history of some of these things seeming like a big deal and then
kind of, and then kind of dissolving some of them are real. I mean, the need for data is real,
maybe continual, continual learning is a real thing. But again, I would ground us in something like
code. Like, I think we may get to the point in like a year or two where the models can just do
sweet and tend. Like, that's a whole task. That's a whole sphere of human activity that we're just
saying models can do it now. When you say end to end, do you mean setting technical direction,
understanding the context of the problem? Yes, et cetera. Yes. So I mean all of that. Interesting.
I mean, that, that is, if you're like, AJ, I complete. Me, there's maybe is internally consistent,
but it's not like saying 90% of code or 100% of code. It's like, no, no, no, no, no, no, no,
I gave this, I gave this spectrum 90% of code, 100% of code, 90% of end-tents, we, 100% of end-tents,
we new tasks are created for trees eventually those get done as well. But the long Spanish
end there. But we're traversing this spectrum very quickly. Yeah. I do think it's funny that
I've seen a couple of podcasts you've done where the hosts will be like, but the orchestra
or the session about the concerto learning thing. And it always makes me crack off because you're like,
you know, you've written a high researcher for like 10 years. I'm sure there's like some
feeling of like, okay, so podcasts are what? And that's it. You know, like every internet
I get asked about it. You know, the truth, the truth of the matter is that we're all trying
to figure this out together. Yeah, right? There are some ways in which I'm able to see things
that others aren't. These days, that probably has more to do with like, I can see a bunch of stuff
within and throw up. And after make a bunch of decisions, then I have any great research insight
that others don't. Right? I, you know, I'm running in 2500 person company like it's actually
pretty hard for me to have a concrete research insight, you know, much harder than, you know,
than it would have been, you know, 10 years ago or, you know, or even two or three years ago.
As we go towards a world of a full drop in remote work or replacement does a API pricing model
still make the most sense. And if not, what is a correct way to price API or serve API?
Yeah, I mean, I think there's going to be a bunch of different business models here sort of
all at once that are going to be that are going to be experimented with. I actually do think that the
API model is is more durable than many people think. One way I think about it is if the
technology is kind of advancing quickly, if it's advancing exponentially, what that means is there's
there's always kind of like a surface area of kind of new use cases that have been developed
in the last in the last three months. And any kind of product surface you put in place is always
at risk of sort of becoming irrelevant, right? And he given product surface probably makes sense
for our, you know, a range of capabilities of the model, right? The the chatbot is already running
into limitations of, you know, making it smarter doesn't really help the average consumer that much.
But I don't think that's a limitation of AI models. I don't think that's evidence that, you know,
the models are the models are good enough and they're there, you know, them getting better doesn't
matter to the economy. It doesn't matter to that particular product. And so I think the value of the
API is the API always offers an opportunity, you know, very close to the bare metal to build on
what the latest thing is. And so there's, you know, there's, there's, there's kind of always going to
be this, you know, this, this kind of front of new startups and new ideas that weren't possible
a few months ago and are possible because the model is advancing. And so I actually, I kind of
actually predict that we are, it's going to exist alongside other models, but we're always going to have
the API business model because there's, there's always going to be a need for a thousand different
people to try experimenting with the model in different way and a hundred of them become startups
and 10 of them become big successful startups and, you know, two or three really end up being the
the way that people use the model of a, of a given generation. So I basically think it's always
going to exist. At the same time, I'm sure there's going to be other models as well, like not every
token that's output by the model is worth the same amount. Think about, you know, what is the value
of the tokens that are like, you know, that the model outputs when someone, you know, someone, you know,
someone, you know, calls them up and says my Mac isn't working or something, you know, the models
like restart it, right? And like, you know, someone hasn't heard that before, but like, you know,
the model said that like 10 million times, right? You know, that maybe that's worth like a dollar or a few
sense or something. Whereas if the model, you know, the model goes to, you know, one of the,
one of the pharmaceutical companies and it says, you know, there's molecule you're developing,
you should take the aromatic ring from that end to the molecule and put it on that end to the
molecule. And, you know, if you do that, wonderful things will happen. Like, like, those tokens
could be worth, you know, 10 million dollars, right? So, so I think we're definitely going to see
business models that recognize that, you know, at some point we're going to see, you know, pay for
results or, you know, in some, in some form or we may see forms of compensation that are like
labor, you know, that that kind of work by the hour. I, you know, I don't know. I think, I think,
I think because it's a new industry, a lot of things are going to be tried. And I, you know,
I don't know what will turn out to be the right thing. What I find, I take your point that people
will have to try things to figure out what is the best way to use this blob of intelligence. But
what I find striking is Claude Code. So, I don't think in the history of startups, there has been
single application that has been as hotly competed in, has coding agents. And, and Claude
Code is a category leader here. And that seems surprising to me. Like, it doesn't seem
intrinsically like anthropic hat to build this. And I wonder if you have an accounting of
why it had to be anthropic or why, how anthropic ended up building an application in addition to
the model underlying it. Yeah. So, it actually happened in a pretty simple way, which is we had our own
you know, we had our coding models, which were good at coding. And, you know, around the
beginning of 2025, I said, I think the time is come where you can have non-trivial acceleration
of your own research if you're an AI company by using these models. And of course, you know,
we, you need an interface and you need a harness to use them. And so I encourage people internally,
and I didn't say, this is one thing that, you know, that you have to use. I just said,
people should experiment with this. And then, you know, this thing, you know, I think it might have
been originally called Claude CLI, and then the name eventually got changed to Claude Code internally,
was the thing that kind of everyone was using. And it was seen fast internal adoption. And I
looked at it and I said, probably we should launch this externally, right? You know, it's seen
such fast adoption within anthropic, like, you know, like, you know, coding is a lot of what we do.
And so, you know, we have an audience of many, many hundreds of people that's in some ways,
at least, representative of the external audience. So it looks like we already have product
market fit. Let's launch this thing. And then we've launched it. And I think, you know, just just
the fact that we ourselves are kind of developing the model. And we ourselves know what we most need
to use the model. I think it's kind of creating this feedback loop. I say, in the sense that you,
let's say a developer at anthropic is like, ah, it would be better if it was better at this
X thing. And then you bake that into the next model that you build. That's that's one version of it.
But then there's just the ordinary product iteration of like, you know, we have a bunch of
bunch of coders within anthropic. Like, we, you know, they like use Claude Code every day. And so we
get fast feedback. That was more important in the early days. Now, of course, there are millions of
people using it. And so we get a bunch of external feedback as well. But it's, you know, it's just
great to be able to get, you know, kind of kind of fast, fast internal feedback. You know,
I think this is the reason why we launched a coding model. And, you know, didn't launch a
pharmaceutical company, right? It, you know, you know, my backgrounds in my backgrounds in, in like
biology. But like, we don't have any of the resources that are needed to launch a pharmaceutical company.
So there's been a ton of hyper-owned open-cloth. And I wanted to check it out for myself.
I'm going to date coming up this weekend, and I don't have anything planned yet. So I gave
open-cloth a mercury debit card. I set a couple hundred dollar limit, and I said, surprise me.
Okay, so here's the Nakmini it's on. And besides having access to my mercury, it's totally quarantined.
Now, as you felt quite comfortable giving you an access to a debit card, because mercury
makes it super easy to set up guard rails. I was able to customize permissions,
cap the spend, and restrict your category of purchases. I wanted to make sure the debit card
worked. So I asked open-cloth to just make a test transaction, and decided to don't
a couple bucks to Wikipedia. Besides that, I have no idea what's going to happen. I will
report back on the next episode about how it goes. In the meantime, if you want a personal baking
solution that can accommodate all the different ways that people use their money, even experimental
ones like this one, visit mercury.com slash personal. Mercury is a fintech company, not an FDIC
intro big. Bigging services provided through choice financial group and column NA members FDIC.
You know, she thinks we're getting coffee and walking around the neighborhood.
Let me ask you about now making AI go well. It seems like whatever vision we have about
how AI goes well has to be compatible with two things. One is the ability to build and run AI's
diffusing extremely rapidly, and two is that the population of AI's, the amount we have in
their intelligence will also increase very rapidly. That means that lots of people will be able
to build huge populations of misaligned AI's, or AI's, which are just companies which are trying
to increase their footprint, or have weird psyche's, like Sydney Bing, but now they're super
human. What is a vision for a world in which we have an equilibrium that is compatible with lots of
different AI's, some of which are misaligned, running around. Yeah, so I think, you know, in the
adolescence of technology, I was kind of, you know, skeptical of like the balance of power.
But I think I was particularly skeptical of, or the thing I was specifically skeptical of is you
have like three or four of these companies like kind of all building models that are kind of dry,
you know, sort of sort of like, like, derived from the, like, derived from the same thing,
and, you know, that these would check each other, or even that kind of, you know, any number of them
would would would would would would check each other like we might live in an off-ends dominant world,
where, you know, like one person or one AI model is like smart enough to do something that like causes damage
for everything else. I think in the, I mean, in the short run, we have a limited number of players now,
so we can start by within the limited number of players. We, you know, we kind of, you know, we, we need
to put in place the, you know, the safeguards. We need to make sure everyone does the right
alignment work. We need to make sure everyone has bio classifiers like, you know, those are those
are kind of the immediate things we need to do. I agree that, you know, that that doesn't solve the
problem in the long run, particularly if the ability of AI models to make other AI models proliferates,
then, you know, the whole thing can kind of, you know, it can become harder to solve. You know,
I think, I think in the long run, we need some architecture of governance, right? Some are some
architecture of governance that preserves human freedom, but, but kind of also allows us to, like,
you know, govern the very large number of kind of, you know, a human systems AI systems,
hybrid, hybrid human, you know, hybrid, hybrid human AI, like, you know, companies, or like,
or like, or like economic units. So, you know, we're going to need to think about, like,
you know, how do we, how do we protect the world against, you know, bioterrorism? How do we protect
the world against, like, you know, against, like, against, like mirror life, like, you know,
probably we're going to need to, you know, need some kind of, like, AI monitoring system that, like,
money, you know, kind of monitors for, for all of these things, but then we need to build this in a way
that, like, you know, preserve civil liberties and, like, our constitutional rights. So I think just,
just as, as is anything else, like, it's, it's like a new security landscape with a new set of,
you know, a new set of tools and a new set of vulnerabilities. And I think my worry is,
if we had 100 years for this to happen all very slowly, we'd get used to it. You know, like,
we've gotten used to, like, you know, the presence of, you know, the presence of explosives in society,
or, like, the, you know, the presence of various, you know, like new weapons, or the, you know,
the presence of video cameras, we would get used to it over, over 100 years. And we develop
governance mechanisms, we'd make our mistakes. My worry is just that this happening also fast.
And so I think maybe we need to do our thinking faster about how to make these governance mechanisms work.
Yeah. It seems like in an offense dominant world, over the course of the next century. So the
idea is the AI is making the progress that would happen in the next century happen in some period
of five to 10 years. But we would still need to see mechanisms, or balance of power would be
similarly intractable, even if humans were the only game in town. And so I guess we have the
advice of AI. We, it fundamentally doesn't seem like a totally different ball game here. If
protection balances were going to work, they would work with humans as well if they aren't going
to work, they wouldn't work with AI as well. And so maybe this is just doom as human checks and
balances as well. But again, I think there's some way to, I think there's some way to make this
happen. Like it, you know, it just, you know, the governments of the world may have to work together
to make it happen. Like, you know, we may have to, you may have to talk to AI is about kind of,
you know, building societal structures in such a way that like these, these defenses are possible.
I don't know. I mean, this is so, this is, you know, I don't want to say so far ahead in time,
but like so far ahead in tech, technological ability that may happen over a short period of time,
that it's hard for us to anticipate in advance. Speaking of governments getting involved,
on December 26th, the 10th C legislature introduced a bill, which said quote,
it would be an offense for a person to knowingly train our official intelligence to provide
emotional support, including through open-ended conversations with the user. And of course,
one of the things that Claude attempts to do is be a thoughtful, thoughtful friend,
thoughtful knowledgeable friend. And in general, it seems like we're going to have this patchwork
of state laws. A lot of the benefits that normal people could experience as a result of AI are going
to be curtail, especially when we get into the kinds of things you discuss in machines of loving
grace, biological freedom, mental health improvements, et cetera, et cetera. It seems easy to imagine
roles in which these get whack them all the way by different laws. Whereas bills like this don't
seem to address the actual existential threats that you're concerned about. So I'm curious about
to understand in the context of things like this, your etthropics position against the federal
moratorium on state AI laws. Yes, so I don't know. There's many different things going on at once,
right? I think that that, I think that particular law is dumb. Like, you know, I think it was
clearly made by legislators who just probably had little idea what AI models could do and not do.
They're like, hey, I model serving as that, that just sounds scary. Like, I don't want, I don't want
that to happen. So, you know, we're not, we're not in favor of that, right? But, but that wasn't the
thing that was being voted on. The thing that was being voted on is we're going to ban all state
regulation of AI for 10 years with no apparent plan to do any federal regulation of AI, which would
take Congress to pass, which is a very high bar. So, you know, the idea that we'd ban states from
doing anything for 10 years, and people said they had a plan for federal government, but, you know,
there was no actual, there was no professional on the table, there was no actual attempt.
Given the serious dangers that I lay out in adolescents of technology around things like, you know,
kind of biological weapons and bioterrorism, autonomy risk, and the timelines we've been talking
about, like, 10 years is an eternity. Like, that's a crazy thing to do. So, if that's the choice,
if that's what you force us to choose, then we're going to choose not to have that moratorium.
And, you know, the benefits of that position exceed the costs, but it's not a perfect position
if that's the choice. Now, I think the thing that we should do, the thing that I would support
is the federal government should step in, not saying states you can't regulate, but here's
what we're going to do, and states you can't differ from this, right? Like, I think preemption is
fine in the sense of saying that federal government says, here's our standards, this applies
to everyone, states can't do something different. That would be something I would support if it
will be done in the right way. But, but this idea of states you can't do anything and we're not
doing anything either. That struck us as, you know, very much not making sense, and I think we'll
not age well was already starting to not age well with all the backlash that you've seen.
Now, in terms of what we would want, I mean, you know, the things we've talked about are
starting with transparency standards, you know, in order to monitor some of these autonomy risks
and bioterrorism risks. As the risks become more serious, as we, as we get more evidence for them,
then I think we could be more aggressive in some targeted ways and say, hey, AI bioterrorism
is really a threat. Let's, let's pass a law that kind of forces people to have classifiers.
And I could even imagine, it depends. It depends how serious the threat it ends up being. We don't
know for sure. Then we need to pursue this in an intellectually honest way, where we say ahead of time,
the risk has not emerged yet. But I could certainly imagine with the pace that things are going,
that, you know, I could imagine a world where later this year, we say, hey, this AI bioterrorism
stuff is really serious. We should do something about it. We should put it in a federal, we should,
you know, put it in a federal standard. And if the federal government won't act, we should put it
in a state state standard. I could totally see that. I'm concerned about a world where
if you just consider the, the pace of progress you're expecting, the life cycle of
legislation. You know, the benefits are, as you say, because a diffusion lag, the benefits are
slow enough that I really do think this patchwork of, on the current trajectory, this patchwork of
state laws would prohibit, I mean, having an emotional chatbot friend is something that freaks people out,
then just imagine the kinds of actual benefits from AI we want, normal people to be able to
experience from improvements in health and health span and improvements in mental health and so forth.
Whereas at the same time, it seems like you think the dangers are already on the horizon and
I just don't see that much. It seems like would be especially injurious to the benefits of AI,
as compared to the dangers of AI. So that's maybe the, where the cost benefit makes less sense
to me. So, so there's a few things here, right? I mean, people talk about there being thousands of
these state laws. First of all, the vast mass majority of them do not pass. And, you know,
the world works a certain way in theory, but just because a law has been passed doesn't mean
it's really enforced, right? The people, the people, you know, implementing it may be like,
oh my god, this is stupid. It would mean shutting off like, you know, everything that's ever been built
and everything that's ever been built in Tennessee. So, you know, very often laws are interpreted in
like, you know, a way that makes them, that makes them not as dangerous or not as harmful on the
same side. Of course, you have to worry if you're passing a law to stop a bad thing, you have this,
you have this problem as well. Yeah. Look, my, my, look, I mean, my basic view is, you know,
if, if, if, you know, we could decide, you know, what laws were passed and how things were done,
which, you know, we're only one small input input into that, you know, I would deregulate a lot of
the stuff around the health benefits of AI. I think, you know, I don't worry as much about the,
like, the, the, the, the kind of chatbot laws. I actually worry more about the drug approval process,
where I think AI models are going to greatly accelerate, um, the rate at which we discover drugs,
and just the pipeline will get jammed up, like the pipeline will not be prepared to like process
all all the stuff that's going through it. So, um, you know, I, I think, I think reform of the
regulatory process to bias more towards, we have a lot of things coming where the safety and the efficacy
is actually going to be really crisp and clear, like, I mean, a beautiful thing that really,
really, really crisp and clear and like really, really effective, but, you know, and, and maybe we
don't need all this, all this, um, uh, like, um, all this superstructure around it, that was designed
around an era of drugs that barely work and often have serious side effects. Um, but at the same time,
I think we should be ramping up quite significantly, the, um, you know, this, this kind of safety
and security legislation, and, you know, like I've said, um, you know, starting with transparency
is, is my view of trying not to hamper the industry, right, trying to find the right balance.
I'm worried about it. Some people criticize my essay for saying, that's too slow. The dangers of AI
will come too soon if we do that. Well, basically, I kind of think like the last six months and maybe
the next few months are going to be about transparency and then if these, if these risks emerge,
we're more certain of them, which I think we might be as soon as, as later this year,
then I think we need to act very fast in the areas that we've actually seen the risk. Like,
I think the only way to do this is to be nimble. Now, the legislative process is normally not nimble,
but we need to emphasize to everyone involved the urgency of this. That's why I'm sending this
message of urgency, right? That's why I wrote adolescents of technology. I wanted policy makers
to read it. I wanted economists to read it. I want national security professionals to read it. You
know, I want decision makers to read it so that they have some hope of acting faster than they
would have otherwise. Is there anything you can do or advocate that would make it's more certain
that the benefits of AI are better and saturated? Where I feel like you have worked with
legislatures to be like, okay, we're going to prevent bioterrorism here away. We're going to
increase the insurgency. We're going to increase risk of blood protection. And I just think by default,
the actual bent, like the things we're looking forward to here, it just seems very easy. They seem
very fragile to different kinds of moral panics or political economy problems. I don't actually,
so I don't actually agree that much in the developed world. I feel like in the developed world
like markets function pretty well. And when there's like a lot of money to be made on something,
and it's clearly the best available alternative, it's actually hard for the regulatory system to
stop it. We're seeing that in AI itself, right? I think I've been trying to fight for as
export controls on chips to China, right? And that's in the national security interests of the
US, like that's like square within the policy beliefs of, almost everyone in Congress,
of both parties. But I think the cases very clear of the counter arguments against it are
I'll politely call them fishy. And yet it doesn't happen and we sell the chips because they're
there's so much money, there's so much money riding on it. And you know, the that money wants to be
made and in that case, in my opinion, that's a bad thing. But it also applies when it's a good thing.
And so I don't think that if we're talking about drugs and benefits of the technology,
I am not as worried about those benefits being hampered in the developed world. I am a little
worried about them going too slow. And as I said, I do think we should work to speed the approval
process in the FDA. I do think we should fight against these chatbot bills that you're describing,
right, described individually. I'm against them. I think they're stupid. But I actually think the
bigger worry is a developing world, where we don't have functioning markets, where we often
can't build on the technology that we've had, I worry more that those folks will get left behind.
And I worry that even if the cures are developed, maybe there's someone in rural Mississippi
who doesn't get it as well, right? That's a kind of smaller version of the thing
the concern we have in the developing world. And so the things we've been doing are, you know,
work with philanthropists, right? We work with folks who deliver medicine and health interventions
to developing world to sub-Saharan Africa, India, Latin America, other developing parts of the
world. That's the thing that I think that won't happen on its own. You mentioned extra controls.
Yeah, why can't the US and China both have a country of geniuses on a bit of a center? Why can't
do you know why won't it happen or why shouldn't it happen? Why shouldn't it happen? You know,
I think if this does happen, you know, then we kind of have a, well, we could have a few situations.
If we have like an offense dominant situation, we could have a situation like nuclear weapons,
but like more dangerous, right? Where it's like, you know, kind of kind of either side could
easily destroy everything. We could also have a world where it's kind of, it's unstable,
like the nuclear equilibrium is stable, right? Because it's, you know, it's like deterrence.
But let's say there were uncertainty about like if the two AI's fought, which AI would win.
That could create instability, right? You often have conflict when the two sides have a different
assessment of their likelihood of winning, right? If one side is like, oh yeah, there's a 90%
chance all win and the other side's like, there's a 90% chance all win, then then a fight is
much more likely. They can't both be right, but they can't both think that. But this is like a
fully general argument against the diffusion of AI technology, which it may, if it's a, that's the
implication of this world. Let me, let me just go, because I think we will get diffusion eventually.
The other concern I have is that people, the governments will oppress their own people with AI.
And so, you know, I'm just, I'm worried about some world where you have a country that's already,
you know, kind of, you know, there's a government that kind of already, you know, is kind of kind
of building a, you know, a high-tech authoritarian state. And to be clear, it's about the government,
it's not about the people like people, we need to find a way for people everywhere to benefit.
My worry here is about governments. So, yeah, my, you know, my worry is that the world gets carved
up into two pieces. One of those two pieces could be authoritarian or totalitarian in a way that's
very difficult to displace. Now, we'll, we'll government's eventually get powerfully AI.
And, you know, there's risk of authoritarianism. Yes, we'll government's eventually get powerfully
AI and there's risk of, you know, of kind of bad, bad, bad equilibrium. Yes, I think both things,
but the initial conditions matter, right? You know, at some point, we're going to need to set up
the rules of the road. I'm not saying that one country, either the United States or a coalition of
democracies, which I think would be a better set up, or it requires more international cooperation
that we currently seem to want to make. But, you know, I don't think a coalition of democracies
or certainly one country should just say these are the rules of the road. There's going to be
some negotiation, right? The world is going to have to grapple with this. And what I would like
is that the, you know, the democratic nations of the world, those with, you know, who's governments
have represented closer to pro-human values are holding the stronger hand than have more leverage
when the rules of the road are set. And so I'm very concerned about that initial condition.
I was releasing to an interview from three years ago, and one of the ways it aged poorly is that
I kept asking questions assuming there is going to be some key, folk or moment, two to three
years or now. When, in fact, being that far out, it just seems like progress continues, AI improves,
AI's more diffuse, and people will use it for more things. It seems like you're imagining a
world in the future where the countries get together and here's the rule of the road and here's
the leverage we have here's the leverage you have. When it seems like on current trajectory,
everybody will have more AI. Some of that AI will be used by authoritarian countries. Some of that,
within the authoritarian countries will be biased by private actors versus state actors. It's
not clear who will benefit more. It's always unpredictable to tell an advance. You know,
it seems like the internet privileged authoritarian countries more than you would have expected.
And maybe the AI will be the opposite way around. So I, I want to better understand what you're
imagining here. Yeah. Yeah. So just to be precise about it, I think the exponential of the
underlying technology will continue as it has before. The models get smarter and smarter. Even
when they get to country of geniuses in the data center, I think you can continue to make the model
smarter. There's a question of like getting diminishing returns on their value in the world.
How much does it matter after you've already solved human biology or, you know, at some point,
you can do harder math. You can do more abstract math problems. But nothing after that matters.
But putting that aside, I do think the exponential will continue. But there will be certain
distinguished points on the exponential and companies, individuals, countries will reach those
points at different times. And so, you know, there's, there's, you know, could there be some,
you know, I, you know, I talk about is a nuclear deterrent still in the lessons of technology,
is a nuclear deterrent still stable. In the world of, of a, I don't know. But that's,
that's an example of like one thing we've taken for granted that like the technology could reach
such a level that it's no longer like, you know, we can no longer be certain of it at least.
You know, think of, think of others, you know, there, there, there are kind of points where,
if you, if you reach a certain point, you may be you have offensive cyber dominance. And like
every, every computer system is transparent to you after that. I, you must the other side
has it has a kind of equivalent defense. So I don't know what the critical moment is or if there's
a single critical moment. But I think there will be either a critical moment, a small number of
critical moments or some critical window where it's like AI is AI can fur some large
advantage from the perspective of national security and one country or coalition has reached it
before others. That, that, you know, I'm not advocating that they're just like, okay, we're in charge
now. That's not, that's not how I think about it. You know, that there's always the other side
is catching up. There's extreme actions. You're not willing to take and, and it's not right to take,
you know, to take complete, to take complete control anyway. But but at, at the point that that
happens, I think people are going to understand that the world has changed and there, there's
going to be some negotiation implicit or implicit about what, what is the, what is the post-day
I world order look like? And, and I think my interest is in, you know, making that negotiation
be one in which, you know, classical liberal democracy has, you know, has a strong hand.
Well, I want to understand what that better means because you say in the essay, quote,
a talkercy is simply not a form of government that people can accept in the post-powerful
AI age. And that sounds that you're saying the CCP as an institution cannot exist after we get
a G.I. And that seems like a, like a very strong demand and it seems to imply a world where the
leading, lab or the leading country will be able to, and by that language should, get to determine
how the world is governed or what kinds of governments are allowed and not allowed.
Yeah, so when, when I, I believe that paragraph was, I think I said something like you could take
even further and say X. So I wasn't, I wasn't necessarily endorsing that, that I wasn't necessarily
endorsing that view. I, you know, I was saying like, here's, first, you know, here's a weaker thing
that I believe, you know, I think, you know, I think I said, you know, we have to worry a lot about
authoritarians and, you know, we should try and, you know, kind of, kind of check them and limit their
power. Like, you could take this kind of further, much more interventionist view that says like
authoritarian countries with AI are these, you know, these kind of self-fulfilling cycles that
you can't, that are very hard to displace and so you just need to get rid of them front from the
beginning, that that has exactly all the problems you say, which is, you know, you know, if you were
to make a commitment to overthrowing every authoritarian country, I mean, they, then they would
take a bunch of actions now that like, you know, that, that could, could lead to instability.
So that, that, that, you know, that, that, that just, that just may not be possible.
But the point I was making that I do endorse is that it is, it is quite possible that, you know,
today, you know, the view or at least my view or the view and most of the Western world is,
democracy is a better form of government than authoritarianism. But it's not like if a country's
authoritarian, we don't react the way we react it if they committed a genocide or something, right?
And, and I'm, I guess what I'm saying is I'm a little worried that in the age of AGI,
authoritarianism will have a different meaning. It will be a grave or thing. And, and we have to decide
when we are another, how, how, how, how, how to deal with that. And the interventionist view is one
possible view. I was exploring such views, you know, may end up being the right view, it may end up
being too extreme to be the right view. But I do have hope. And, and one piece of hope I have is,
there is, we have seen that as new technologies are invented, forms of government become obsolete.
I, I mentioned this in adolescence of technology where I said, you know, like feudalism was
basically, you know, like a form of government, right? And, and then when, when we invented industrialization,
feudalism was no longer sustainable, no longer made sense. Why is that hope? Well,
couldn't that imply that democracy is no longer going to be, well, competitive systems?
It could, right, it could go, it could go either way, right? But, but I actually, so
I, these problems with authoritarianism, right, that the problems of the authoritarianism get deeper,
I just, I wonder if that's an indicator of other problems that authoritarianism will have,
right? In other words, people become because authoritarianism becomes worse. People are more afraid
of authoritarianism. They work harder to stop it. It's, it's more of a, like, you have to think
in terms of totally equilibrium, right? I just wonder if it will motivate new ways of thinking about
with with with the new technology, how to preserve and protect freedom. And, and even more
optimistically, will it lead to a collective reckoning and, you know, a kind of more emphatic realization
of how important some of the things we take as individual rights are, right? A more emphatic
realization that we just, we really can't give these away. There's, there, we've seen, there's no
other way to live that actually works. I, I, I, I, I, I am actually, I am actually hopeful that
I, I guess one way to say it, it sounds too idealistic, but I actually believe it could be the case,
is that, is that dictatorships become morally obsolete. They become morally unworkable forms of
government, um, and that, and that, and that, the, the, the, the crisis that that creates is, is,
is sufficient to force us to find another way. Um, I, I think there is genuinely a tough question
here, which I'm not sure how you resolve, and we've had to come out one where another on
a, through history, right? So with China and the 70s and 80s we decided, even though it's an
authoritarian system, we will engage with it. I think it's right, that was the right call,
because it's a state authoritarian system, but a billion plus people are much wealthier and
better off than they would have otherwise been. Um, and it's not clear that it would have
stopped being an authoritarian country otherwise. You can just look at North Korea, uh, as an example
of that, right? And I don't know if that, that, that, that much intelligence to remain in the
authoritarian country that continues to coalesce its own power. And so you can just imagine a
North Korea with an AI that's much worse than everybody else's, but still enough to keep power.
And, and, and, and then, and then, so in general, it seems like,
should we just have this attitude of the benefits of AI will, in the form of all these
empowerments of humanity and health and so forth, will be big. And in historically, we have decided
it's good to spread the benefits of technology widely, even with even to people whose governments
are authoritarian. And I think, I guess it is a tough question with how to think about it
with the AI, but, um, historically, we have said, yes, this is a positive some world,
and it's still worth diffusing the technology. Yeah, so, so there are a number of choices we have.
I, you know, I think framing this as a kind of government to government decision, and, you know,
in national security terms, that's like one lens, but there are a lot of other lenses, like
you could imagine a world where, you know, we produce all these cures to diseases and like,
the, you know, the, the cures to diseases are fine to sell to authoritarian countries,
the data centers just aren't, right? The chips and the data centers just aren't.
And, and that the AI industry itself, um, you know, like, like another possibility is, and I think
folks should think about this, like, you know, could there be developments we can make,
either that naturally happen as a result of AI, or that we could make happen by building technology
on AI. Could we create an equilibrium where where it becomes infeasible for authoritarian countries
to deny their people kind of private use of the benefits of the technology? Um, you know,
are there, are there, are there, are there, are there equilibrium where we can kind of give everyone
an authoritarian country their own AI model that kind of, you know, you know, like defend themselves
from surveillance, and there isn't a way for the authoritarian country to like crack, crack down
on this while while retaining power. I don't know, that, that sounds to me like if that went far enough,
it would be, it would be a reason why authoritarian countries would disintegrate from the inside.
Um, but, but maybe there's a middle world where like there's an equilibrium where if they
want to hold on to power of the authoritarian's can't deny kind of individualized access
access to the technology. But I actually do have a hope for the, for the, um, for the, for the
more radical version, which is, you know, is it possible that the technology might inherently have
properties or that by building on it in certain ways we could create properties, um, that, that, that,
that have this kind of dissolving effect on authoritarian structures. Now, we, we, we hope
originally, right? We think that we're back to the beginning of the Obama administration. We thought
originally that that, you know, social media and the internet would have that
property turns out not to, but, but I, I don't know, what, what if we could, what if we could
try again with, with the knowledge of how many things could go wrong and that this is a different
technology? I don't know that it would work, but it's worth the try. Yeah. I think it's just
it's very unpredictable. Like there's first principles of reasons why authoritarian is a very
unpredictable. I don't think, I mean, we got it. We, we just got it. We kind of, we got to recognize
the problem and then we got to come up with 10 things we can try and we got to try those and then
assess whether they're working or which ones are working if any and then try new ones if the
old ones are. But I guess whether that's out to today, as you say, we will not sell data centers,
of course, our chips and then the ability to make chips to China. And so in some sense, you are
denying there will be some benefits to the Chinese economy, Chinese people, etc, because we're doing
that. And then there will also be benefits to the American economy because it's a positive
some world, we could trade, they could have their country data centers doing one thing, we could have
ours doing another. And already we, you're saying it's not worth that positive sub stipend to
empower this country. What, what I would say is that, you know, we are about to be in a world
where growth and economic value will come very easily, if, right, if we're able to build these
powerfully eye models, growth and economic value will come very easily. What will not come
easily is distribution of benefits, distribution of wealth, political freedom. You know,
these are the things that are going to be hard to achieve. And so when I think about policy,
I think I think that the technology and the market will deliver all the fundamental benefits,
you know, almost almost faster than we can take them. And that these questions about
distribution and political freedom and rights are the ones that that will actually matter
and that policy should focus on. Okay, so speaking of distribution, as you're mentioning,
we have developing countries and in many cases, catch up growth has been weaker than we would
have hoped for. But when catch up growth does happen, it's fundamentally because they have
underutilized labor, you can bring the capital and know how from developed countries to these
countries. And then they can grow quite rapidly. Yes, obviously in a world where labor is no longer
the constraining factor, this mechanism no longer works. It's just the hope, basically, to rely on
philanthropy from the people who immediately get wealthy from AI or from the countries that
go out and rely on it. What is it? I mean, I mean, philanthropy should obviously play some role,
as it has, you know, as it has, as in the past. But I think growth is always, growth is always
better and stronger if we can make it endogenous. So, you know, what are the relevant industries
in like, in like, in like, in like, in AI-driven world? Look, there's lots of stuff, you know,
like there's, you know, I said, I said, we shouldn't build data centers in China, but there's no
reason we shouldn't build data centers in Africa, right? In fact, I think it'd be great to build
data centers in Africa. You know, as long as they're not owned by China, we should build,
we should build data centers in Africa. I think that's a, that's a, that's a, I think that's a great
thing to do. You know, we should also build, you know, there's no reason we can't build,
you know, a pharmaceutical industry that's like AI-driven, like, you know, the, the, if AI is accelerating,
accelerating, drug discovery, then, you know, there will be a bunch of biotech startups, like,
let's make sure some of those happen in the developing world. And certainly, during the transition,
I mean, we can talk about the point where humans have no role, but, but humans will have still
have some role in starting up these companies and supervising, supervising the AI models. So,
let's make sure some of those humans are humans in the developing world, so that fast growth
can happen there as well. You guys recently announced, Quad is going to have a constitution that's
a line to set a value is, and not necessarily just the end user. And there's a world that
can imagine where if it is a line to the end user, it preserves the balance of how we have in the world
today, because everybody gets to have their own AI. That's advocating for them. And so the
ratio of bad actors are good actors, stays constant. It seems to work out for our world today.
Why is it better not to do that, but to have a specific set of values that the AI should
carry forward? Yeah, so I'm not sure I'd quite draw the distinction in that way. There are maybe
two relevant distinctions here, which are, I think you're talking about a mix of the two.
Like, one is, should we give the model a set of instructions about
do this and versus don't do this? Yeah. And the other, you know, versus should we give the model
a set of principles for, you know, for a kind of how to act? And there it's, you know, it's you know,
it's it's just purely a practical and empirical thing that we've observed that by teaching the
model principles, getting it to learn from principles, it's behavior is more consistent,
it's easier to cover edge cases. And the model is more likely to do what people want it to do.
In other words, if you, you know, if you're like, you know, don't tell people how to hotwire a car,
don't speak in Korean, don't, you know, you know, just, you know, if you give it a list of rules,
it doesn't really understand the rules and it's kind of hard to generalize from them.
You know, if it's just kind of a like, you know, list of do-do's and don'ts, whereas if you give it
principles and then, you know, it has some hard guardrails like don't make biological weapons.
But overall, you're trying to understand what it should be aiming to do, how it should be aiming
to operate. So just from a practical perspective, that turns out to be just a more effective way to trade
in the model. That's one piece of it. So that, you know, it's the kind of rules versus principles
trade-off. Then there's another thing you're talking about, which is kind of like the corageability
versus, like, you know, I would say, kind of intrinsic motivation trade-off, which is like,
how much do the model be a kind of, I don't know, like a skin suit or something, where, you know,
you know, you know, you just kind of, you know, it just kind of directly follows the instructions
that are given to it by whoever is giving it those instructions. First is how much should the model
have an inherent set of values and go off and do things on its own? And there, I would actually
say everything about the model is actually closer to the direction of like, you know, it should
mostly do what people want. It should mostly follow the answer. We're not trying to build something
that like, you know, goes off and runs the world on its own. We're actually pretty far on the
corageable side. Now, what we do say is there are certain things that the model won't do, right,
that it's like, you know, that that, I think we say it in various ways in the constitution,
that under normal circumstances, if someone asks the model to do a task, you should do that task.
That should be the default. But if you've asked it to do something dangerous, or if you've, you know,
if you've asked it to, you know, to kind of harm someone else, then the model is unwilling to do that.
So I actually think of it as like a mostly, a mostly corageable model that has some limits,
but those limits are based on principles. Yeah. I mean, then the fundamental question is,
how are those principles determined? And this is not a special question for anthropic. This
would be a question for any, but because you have been the ones to actually write down the principles,
I get to ask you this question. Normally, a constitution is like, you write it down and it's
set in stone and there's a process of updating it and changing it and so forth. In this case,
it seems like a document that people inthropic write that can be changed at any time, that guides
the behavior of systems that are going to be the basis of a lot of economic activity.
What is the, how do you think about how those principles should be set? Yes. So I think there's,
there's two, there's maybe three, three kind of sizes of loop here, right? Three, three ways to iterate.
One is you can iterate. We iterate within thenthropic. We train the model. We're not happy with it
and we kind of change the constitution. And I think that's good to do. And, you know, putting
out publicly, you know, making updates to the constitution every once in a while saying,
here's a new constitution, right? I think that's good to do because people can comment on it.
The second level of loop is different companies will have different constitutions.
And, you know, I think it's useful for like anthropic, what's out of constitution and, you know,
Gemini model puts out a constitution and, you know, other companies put out a constitution and then
make it kind of look at them, compare outside observers, can critique and say this, I like this one,
this thing from this constitution and this thing from that constitution and then kind of
that, that creates some kind of, you know, soft incentive and feedback for all the companies to
like take the best of each elements and improve. Then I think there's a third loop which is,
you know, society beyond the AI companies and beyond just those who kind of, you know,
who comment on the constitutions without hard power. And there, you know, we've done some
experiments like, you know, a couple years ago, we did an experiment with, I think it was called
the collective intelligence project to like, you know, to basically pull people and ask them what
should be in our AI constitution. And, you know, I think at the time we incorporated some of those
changes. And so you could imagine with the new approach we've taken to the constitution
doing something like that. It's a little harder because it's like, that was actually an easier
approach to take when the constitution was like a list of dues and don'ts. At the level of
principle to ask to have a certain amount of coherence. But you could, you could still imagine
getting views from a wide variety of people. And I think you could also imagine, and this is like a
crazy idea, but hey, you know, this whole interview is about crazy ideas, right? So, you know,
you could even imagine systems of kind of representative government having, having input, right?
Like, you know, I wouldn't, I wouldn't do this today because a legislative process is so slow.
Like, this is exactly why I think we should be careful about the legislative process and the
AI regulation. But there's no reason you couldn't in principle say like, you know, all AI, you know,
all AI models have to have a constitution that starts with like these things. And then like you
can append, you can append other things after it, but like there has to be this special section that
like takes present. I wouldn't do that. That's too rigid. That sounds, you know, that sounds kind of
overly prescriptive in a way that I think overly aggressive legislation is. But like that is a thing
you could, you know, like, that is a thing you could try to do. Is there some much less heavy
handed version of that, maybe? I really like control loop, too. Where obviously this is not how
constitutions of actual governments do or should work, where there's not this vague sense in which
the Supreme Court will feel out how people are feeling and where the vise and then update the
constitution accordingly. So there's, yeah, with actual governments, there's a more procedural
process. Yeah, exactly. But you actually have a vision of competition between constitutions,
which is actually very reminiscent of how some libertarian charter citizen people you used to talk
about and archipelago of different kinds of governments that look like. And then there would be
selection among them of who could operate the most effectively, which in which place people would
be the happiest. And in a sense, you're actually, yeah, there's this vision. I'm kind of recreating
that. Yeah, yeah, like in the sake of utopia of archipelago is, you know, again, you know, I think
that vision has, you know, if things to recommend it and things that things that things that
will kind of go wrong with it, you know, I think it's, I think it's an interesting and some ways
compelling vision, but also things will go wrong with it that you hadn't that you hadn't
imagined. So, you know, I like loop two as well, but I feel like the whole thing has got to be
some mix of loops one, two, and three, and it's a matter of the proportions, right? I think that's
got to be the answer. When somebody eventually writes the equivalent of the, making of the atomic
bomb for this era, what is the thing that will be hardest to glean for the historical record? They're
most likely to miss. I think a few things, one is at every moment of this exponential, the extent
to which the world outside it didn't understand it. This is, this is a bias that's often
present in history where anything that actually happened looks inevitable in retrospect. And, and so,
you know, I think when people, when people look back, it will be hard for them to put themselves
in the place of people who are actually making a bet on this thing to happen that
wasn't inevitable that we had these arguments, like the arguments that I make for scaling or that
continual learning will be solved. That, you know, some of us internally in our heads put a high
probability on this happening, but it's like there's a world outside us that's not acting on acting
on that at all. And, and I think the the weirdness of it, I think unfortunately like the
insularity of it, like, you know, if we're one year or two years away from it, it happened
like the average person on the street has no idea. And that's one of the things I'm trying to change,
like with the memos, with talking to policymakers, but like, I don't know, I think, I think that's
just a that's just like a crazy, that's just like a crazy thing. Yeah. Um, finally, I would say,
and this probably applies to almost all historical moments of crisis, um, how absolutely
fast it was happening, how everything was happening all at once. And so decisions that you might think,
you know, we're kind of carefully calculated. Well, actually you have to make that decision, and then
you have to make 30 other decisions on the on the same day, because it's all happening so fast. And
and you don't even know which decisions are going to turn out to be consequential. So, you know,
one of my, one of my, I guess worries, although it's also an insight into into, you know,
into kind of what's happening is that, you know, some very critical decision will be,
will be some decision that, you know, someone just comes into my office and is like,
Dario, you have two minutes, like, you know, should we, should we do, you know, should we do
think thing A or thing B on this, like, you know, someone gives me this random, you know,
half page, half page memo, and it's like, should we should we do A or B? And I'm like, I don't know,
I have to eat lunch. Let's do B. And, you know, that ends up being the most consequential thing
ever. So, final question, it seems like you have, there's not tech CEOs who are usually
writing 50 page memos every few months. And it seems like you have managed to build a rule for
yourself and accompanying around you, which is compatible with this more intellectual type,
roll a CEO. And I want to understand how you construct that and how, like, how does that work,
to be, you just go away for a couple of weeks and then you tell your company, this is the memo,
like, here's what we're doing. It's also reported that you write a bunch of these internally.
Yeah, so, I mean, for this particular one, you know, I wrote it over winter break.
So, that was the type, you know, and I was having a hard time finding the time to actually find it,
to actually write it. But I actually think about this in a broader way. I actually think it relates
to the culture of the company. So, I probably spend a third, maybe 40% of my time making
sure the culture of anthropic is good. As anthropic has gotten larger, it's gotten harder to just,
you know, get involved in, like, you know, directly involved in, like, the train of the models,
the launch of the models, the building of the products. Like, it's 2,500 people. It's like,
you know, there's just, you know, I have certain instincts, but like, there's only, you know,
you know, it's very difficult to get to get involved in every single detail. You know,
I like, I try as much as possible. But one thing that's very leveraged is making sure
anthropic is a good place to work. People like working there. Everyone thinks that
themselves as team members have one works together instead of against each other. And, you know,
we've seen as some of the other AI companies have grown without naming any names. You know,
we're starting to see decoherence and people fighting each other. And, you know, I would argue
there was even a lot of that from the beginning, but, you know, that it's, it's gotten worse. But
I think we've done an extraordinarily good job, even if not perfect, of holding the company together,
making everyone feel the mission that we're sincere about the mission and that, you know,
everyone has faith that everyone else there is working for the right reason that we're a team
that people aren't trying to get ahead of each other's expense or backstab each other, which, again,
and happens a lot at some of the other places. And, and how do you make that the case? I mean,
it's a lot of things. You know, it's me, it's, it's Daniyala who, you know, runs the company day to
day. It's the co-founders. It's the other people we hire. It's the environment we try to create.
But I think an important thing in the culture is I, some, you know, the other leaders as well,
but especially me, have to articulate what the company is about, why it's doing, what it's doing,
what it's strategy is, what its values are, what its mission is and what it stands for.
And, you know, when you get to 2,500 people, you can't do that person by person.
You have to write or you have to speak to the whole company. This is why I get up in front of
the whole company every two weeks and speak for an hour. It's actually, I mean,
I wouldn't say I write essays internally. I do two things. One, I write this thing called DVQ,
Dario Vision Quest. I wasn't the one who named it that, that's the name, it received.
And it's one of these names that I kind of, I tried to fight it because it made it sound like I was
like going off and smoking pale years. But, but the name just stuck. So, I get up in front of the
company every two weeks. I have like a three or four page document and I just kind of talk through
like three or four different topics about what's going on internally. The, you know, the models
were producing the products, the outside industry, the world as a whole, as it relates to AI and
geopolitically in general, you know, just some mix of that. And I just go through very, very honestly,
I just go through and I just, I just say, you know, this is what I'm thinking. This is what
anthropically leadership is thinking. And then I answer questions. And, and that direct connection,
I think has a lot of value that is hard to achieve when you're passing things down the chain,
you know, six, six levels deep. And, you know, a large fraction of the company comes to attend,
either either in person or either in person or virtually. And, you know, it really means that
you can communicate a lot. And then the other thing I do is I just, you know, I have a channel
and Slack, or I just write a bunch of things and comment a lot. And often that's in response to,
you know, just things I'm seeing at the company or questions people ask or like, you know,
we do internal surveys and there are things people are concerned about and so I'll write them up.
And I'm like, I'm, you know, I'm, I'm just, I'm very honest about these things. You know, I just,
I just say them very directly. And the point is to get a reputation of telling the company
to truth about what's happening, to call things what they are, to acknowledge problems,
to avoid the sort of corpospique, the kind of defensive communication that often is necessary in
public because, you know, the world is very large and full of people who are, you know,
interpreting things in bad faith. But, you know, if you have a company of people who you trust,
and we try to hire people that we trust, then, then, you know, you can, you can, you know,
you can, you can really just be entirely unfiltered. And, you know, I think, I think that's an enormous
strength of the company. It makes it a better place to work. It makes people more, you know,
more of the sum of their parts. It increases the likelihood that we accomplish the mission,
because everyone is on the same page about the mission. And everyone is debating and discussing
how it best to accomplish the mission. Well, in lieu of an external adoration quest, we have this interview.
This interview is a little like that. This is in front of you. Thanks for doing it.
Yeah. Thank you, Dr. Cash. Hey, everybody. I hope you enjoyed that episode. If you did,
the most helpful thing you can do is just share it with other people who you think might enjoy.
It's also helpful if you leave a rating or comment on whatever platform you're listening on.
If you're interested in sponsoring the podcast, you can reach out at
porcash.com slash advertisements. Otherwise, I'll see you in the next one.
